---
title: "Springboardâ€™s Introduction to Data Science (Capstone Project) - Predicting Semiconductor Wafer Failures From Process Equipment Sensor Data"
author: "John Campi"
date: "March, 2019"
output:
  html_document:
    highlight: tango
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 4
    toc_float:
      
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 4
linkcolor: blue
geometry: left=2.5cm,right=2.5cm,top=2cm,bottom=2cm
citecolor: blue
urlcolor: blue
fig_caption: true
abstract: "The goal of this project was to create statistical and machine learning models for equipment sensor data from a semiconductor fab to predict wafer pass/fail functionality as a means to improve fab yield. The models explored were logistic regression, modified logistic regression and random forest. The best predictive model using the fewest number of sensor features was found to be iterated logistic regression with class balanced data using a SMOTE method."
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = T)

# Set options.
options(max.print=2500)
#options(na.action = "na.exclude")
```
\newline
\pagebreak

```{r initialization, echo=F, message=F, warning=F, results='hide'}
# load required libraries
library(dplyr, warn.conflicts=F, quietly=T)         # data wrangling
library(tidyr, warn.conflicts=F, quietly=T)         # data wrangling
library(ggplot2, warn.conflicts=F, quietly=T)       # plotting
library(knitr, warn.conflicts=F, quietly=T)         # tables
library(kableExtra, warn.conflicts=F, quietly=T)    # tables
library(naniar, warn.conflicts=F, quietly=T)        # missing data analysis
#library(simputation, warn.conflicts=F, quietly=T)
library(mice, warn.conflicts=F, quietly=T)          # imputation
#library("ImputeRobust")
#library("gamlss")
library(RColorBrewer, warn.conflicts=F, quietly=T)  # for plotting
library(smotefamily, warn.conflicts=F, quietly=T)   # class rebalancing
library(corrplot, warn.conflicts=F, quietly=T)      # correlation heatmap
library(Hmisc, warn.conflicts=F, quietly=T)         # correlation analysis
library(caTools, warn.conflicts=F, quietly=T)       # training testset split
library(arm, warn.conflicts=F, quietly=T)           # bayesglm
library(randomForest, warn.conflicts=F, quietly=T)  # random forest
library(caret, warn.conflicts=F, quietly=T)         # Classification and Regression Training
library(ROCR)                                       # Visualizing the Performance of Scoring Classifiers

#  Set control flags. Prior saved data uses when set to "F".
transformFlag <- T  # run distribution transforms? (T/F)
imputeFlag <- F     # run imputation of missing data? (T/F)
smoteFlag  <- T     # run class rebalancing? (T/F)
fitRfFlag <- F      # fit or read RF model files? (T) or RF fit stats csv file (F) 

# Constants
varUnique  <-  1.0  # min.% unique values per variable filter criteria
maxMissing <- 10.0  # max.% missing per variable filter criteria
threshold <- 0.5    # level threshold for dependent variable Status
maxSig <- 0.05      # max feature significance criteria for regression 
set.seed(123)
```


```{r define functions, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# distMatrix
#-----------------------------------------------------------------------
# Function that plots n x n matrices of distribution density plots for 
# selected variables.
#   - data        dataset name
#   - plotVars    vector of variable names to be plotted
#   - factorBy    vector of factor variable names for overplotting (default="")
#   - plotDim     # of plots per side (plotDim x plotDim matrix, default=1)
#   - nStart      starting variable plot (default=1)
#   - nPlot       max # of plots, default is plot all variables (defaults to all)
#   - nBins       # of bins per plot (default=100)
#   - xScale      logarithmic="log10" else linear (default)
#   - numXaxes     > 1: plot linear & log x-axes, o.w. follow xScale (default)
#   - title       user specifiec title text
#   - hist        histogram? (T/F)
#   - density     density outline? (T/F)
#-----------------------------------------------------------------------
distMatrix <- function(data, plotVars="", factorBy="", plotDim=1, nStart=1, nPlot=length(plotVars)-nStart+1, 
                       nBins=100, xScale="lin", numXaxes=1, title="", QQplot=FALSE, hist=TRUE, density=TRUE) {

    nPlotFrames <- ceiling(nPlot/plotDim^2)                        # total # of plot frames
    firstVar <- nStart                                             # variable # in 1st plot position
    if (xScale == "lin") {
        titleInit = if (title=="") {"Density vs. Var"} else {title}
    } else {
        titleInit = if (title=="") {"Density vs. log(Var)"} else {title}
    }
    
    print(paste("nPlot =", nPlot, "nPlotframes=", nPlotFrames))

    for (n in 1:nPlotFrames) {
        lastVar = min(firstVar + plotDim^2 - 1, length(plotVars))  # variable # in last plot position

        for (m in 1:numXaxes) { 
            if (m > 1) {
                data[plotVars[firstVar:lastVar]] <- abs(data[plotVars[firstVar:lastVar]])          # density vs. log10[abs(var)]
                #data[data == 0] <- 1e-15                          # clean log(0) issue
                if (data[plotVars[firstVar:lastVar]] == 0) {data[plotVars[firstVar:lastVar]] <- 1e-15}
                
                title = "Density vs. log(Var)"
            } else {title = titleInit}
          
            if (factorBy == "") {                                  # no factoring

                p <- data %>%
                  dplyr::select(plotVars[firstVar:lastVar]) %>%
                    gather(key="var", value="value", plotVars[firstVar:lastVar]) %>%
                    ggplot(aes(x=value)) +
                      facet_wrap(~ var, scales = "free") +
                      ggtitle(title) 
                if (hist) {p <- p +  
                    geom_histogram(aes(y = ..density..), 
                                   bins = nBins,
                                   position = "identity",
                                   alpha = 0.8,
                                   color = "blue") 
                } #end if (hist)
                if (density) p <- p + geom_density(alpha = 0.4)
                if (xScale == "log10" | m > 1) p <- p + scale_x_log10()
                print(p)

                if (QQplot) {
                    # Q-Q Plot
                    print(data %>%
                        dplyr::select( plotVars[firstVar:lastVar]) %>%
                          gather(key="var", value="value", plotVars[firstVar:lastVar]) %>%
                          ggplot(aes(sample=value)) +
                            geom_qq(size=2) +
                            geom_qq_line(size=1.5, color="red") +
                            facet_wrap(~ var, scales = "free") +
                            ggtitle("Q-Q Plot")
                    ) #end print
                } #end if
              
            } else {                                               # with factoring
              
                p <- data %>%
                  dplyr::select( factorBy, plotVars[firstVar:lastVar]) %>%
                    gather(key="var", value="value", plotVars[firstVar:lastVar]) %>%
                    ggplot(aes(x=value, color=as.factor(!! sym(factorBy)))) +
                      theme_bw() +
                      #scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9")) +
                      scale_color_brewer(palette="Set1", type="div") +    # colors
                      scale_fill_brewer(palette="Set1", type="div") + # fill
                      facet_wrap(~ var, scales = "free") +
                      ggtitle(title) 
                if (hist) {p <- p + 
                             geom_histogram(aes(y = ..density..), 
                                            bins = nBins,
                                            position = "identity",
                                            #position = "dodge",
                                            alpha = 0.1)  
                }  #end if (hist)
                if (density) p <- p + geom_density(size=1, fill=NA, alpha = 0.1) 
                if (xScale == "log10" | m > 1) p <- p + scale_x_log10()
                print(p)
                
            }#end if (factorBy == "")
            
            print(paste("Plotting", firstVar, "through", lastVar))
            
        } #end for (m in 1:numXaxes)
        
        firstVar <- lastVar + 1                                    # 1st plot variable in next frame
        
        print(paste("firstVar at end of n loop =", firstVar))
        
    } #end for (n in 1:nPlotFrames)
} #end function


#-----------------------------------------------------------------------
# flattenCorrMatrix
#-----------------------------------------------------------------------
# Function that flattens the correlation and p-value matrices into a  
# flat list.
#   - cormat      matrix of the correlation coefficients
#   - pmat        matrix of the correlation p-values
#-----------------------------------------------------------------------
flattenCorrMatrix <- function(cormat, pmat) {
    ut <- upper.tri(cormat)
    var1 <- var2 <- coeff <- p <- NULL
    count <- 1
    for (i in 1:nrow(cormat)) {
        for (j in 1:ncol(cormat)) {
            if (ut[i,j]) {
                var1[count]  <- rownames(cormat)[i]
                var2[count]  <- colnames(cormat)[j]
                coeff[count] <- cormat[i,j]
                p[count]     <- pmat[i,j]
                count = count + 1
            }
        }
    }
data.frame(var1,var2,coeff,p)
}


#-----------------------------------------------------------------------
# plotFitStats
#-----------------------------------------------------------------------
# Function that plots vector yList[j] vs vector xList[i] by vector byVarList[i] 
# faceted xFacetList[i] x yFacetList[i].
#   - data        dataset name
#   - xList       vector of x-axis variable names [common index]
#   - yList       vector of y-axis variable names 
#   - byVarList   vector of plot parameters [common index]
#   - xFacetList  row facet parameters [common index] 
#   - yFacetList  column facet parameters [common index] 
#   - gatherList  vector of variable names to gather in byVar
#   - xScale      logarithmic="log10" (default), linear="lin"
#   - yScale      logarithmic="log10" (default), linear="lin"
#-----------------------------------------------------------------------
plotFitStats <- function(data, xList="", yList="", byVarList="", xFacetList="", yFacetList="", gatherList="",xScale="log10", yScale="log10") {
  
    myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
                   brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    
    for (i in 1:length(xList)) {
        for (j in 1:length(yList)) {
            title <- paste(yList[j],"vs",xList[i],"by",byVarList[i],"(row =",xFacetList[i],"x col =",yFacetList[i],") grid",sep=" ")
             
            if (!(gatherList[1] == "")) {
                p <- data %>% 
                  gather(key="var", value="value", gatherList) %>%
                  ggplot(aes(x=!! sym(xList[i]), y=value, col=as.factor(var)))
            } else {
                p <- data %>%  
                  ggplot(aes(x=!! sym(xList[i]), y=!! sym(yList[j]), col=as.factor(!! sym(byVarList[i])))) 
            }
            
            p <- p +
                geom_line(size=1) +
                geom_point() +
                scale_color_manual(values = myCol) +
                facet_grid(vars(!! sym(xFacetList[i])), vars(!! sym(yFacetList[i]))) +
                ggtitle(title)
              
            if (xScale == "log10") {p <- p + scale_x_log10() } 
            if (yScale == "log10") {p <- p + scale_y_log10() } 
            
            print(p)
        } #end for (i)
    } #end for (j)
} #end function


#-----------------------------------------------------------------------
# tune_threshold
#-----------------------------------------------------------------------
# Function that plots TPR, TNR, accuracy and F1 score vs. threshold.
#   - data        vector of predicted probabilities
#   - reference   vector of reference 2 class values
#-----------------------------------------------------------------------
tune_threshold <- function(data="",reference="") {
    
    tStart <-  0.001                                                       # threshold start
    tStop  <-  1                                                           # threshold stop
    tPerDec  <-  25                                                        # number threshold steps per decade
    nSteps <- tPerDec*log10(tStop/tStart) + 1                              # total steps
    
    stats <- data.frame(TPR = double(),
                        TNR = double(),
                        ACC = double(),
                        F1 = double(),
                        stringsAsFactors = FALSE)
    
    for (i in 1:nSteps) {
        
        threshold <- tStart*10^((i-1)/tPerDec)                             # threshold value
        tempData <- data
        tempData[tempData < threshold] <- 0                                # reset "pass" level
        tempData[tempData >= threshold] <- 1                               # reset "fail" level

        # Confusion Matrix
        cm <- caret::confusionMatrix(data = as.factor(tempData), reference = as.factor(reference))
        ACC <- round(cm$overall[1],4)                                      # accuracy
        TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
        TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
        F1  <- round(cm$byClass[7],4)                                      # F1 score
        
        stats <- base::rbind(stats,c(threshold,TPR,TNR,ACC,F1))
        
    } #for (threshold in 1:nSteps)
    
    colnames(stats) <- c("threshold","TPR","TNR","ACC","F1")

    # plot stats vs. threshold
    myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
                   brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    breaks <- 10^(-10:10)
    minor_breaks <- rep(1:9, 21)*(10^rep(-10:10, each=9))
    
    p <- stats %>%  
        gather(key="var", value="value", TPR,TNR,ACC,F1) %>%
        ggplot(aes(x=threshold, y=value, col=var)) +
          geom_line(size=1) +
          geom_point() +
          scale_color_manual(values = myCol) +
          scale_x_log10(breaks = breaks, minor_breaks = minor_breaks)
    print(p)
    
    i <- nrow(stats)
    while (stats$TNR[i] < stats$TPR[i] & i > 0)  i <- i - 1
    
    round(stats$threshold[i],4)
}

```


```{r summary, eval=T, echo=F, message=F, warning=F}

path_in <- file.path("Data", "summary.csv")

if (file.exists(path_in)) {
    model_summary <- read.csv(path_in, stringsAsFactors=F) 
    model_summary <- within(model_summary, rm(X))                          # drop extra variable   
    model_summary$Stage <- NULL                                            # drop variable

    kable(model_summary) %>%
      kable_styling(bootstrap_options = "striped", full_width = F)
} #end if 

```
\newline

<font size="6pt"; align="center"> **Acknowledgements** </font>

Thank you to the SpringBoard team for providing a great course and learning framework to build off of. Special thank you to my mentor, Blaine Bateman, without whom I would not have learned so much in so little time.

\newline


# Introduction
In this age of electronics, a major driving force is the supply of low cost semiconductor chips. Chip manufacturing occurs in facilities called wafer fabs in which silicon wafers up to widths of 300mm are subjected to a series of processing steps that implant, deposit and etch the microelectronic circuits onto the wafer surface in a 3-dimensional construct. Each of these process steps is performed by various tools that are fitted with sensors to measure their specific operation. Traditionally these sensor data are used by the equipment operator to detect malfunctions as close to real time as possible. If an error condition is detected the operator can quickly make adjustments or stop the process to spare corrupting the existing wafer product. Controlling costs associated with material waste is a major concern for fabs. The measure of overall wafer throughput is called the fab yield (yield) and is calculated as the ratio of the number of functional wafers output from the fab divided by the total number of wafers started. The goal of the fab is to maximize yield by reducing waste by any means. One way to do this is better utilize the equipment sensor data to identify the key instrument sensors of a semiconductor manufacturing line and model the combined alarm conditions for potential wafer failures. This capability would be sought out by any semiconductor manufacturer who wants to maximize their yield. Currently wafers are tested after specific process levels are completed. Much of the critical testing can't be performed until very far along in the manufacturing process. Thus a failure early in the process will consume costly resources as the wafer continues onto other fabrication steps, only to be scrapped at the end. If statistical sampling is used at final test then there's also the added risk of failing chips shipping to customers. Having the ability to catch failures during any process step minimizes the chance of failure propagation and improves isolation time of equipment issues thereby greatly improving yield and reducing costs.

The data used for this project will be the [**SECOM Data Set**](http://archive.ics.uci.edu/ml/datasets/secom) which is publicly available from the UCI archive. The dataset is comprised of 1567 observations, one observation per wafer fabricated, 591 variables corresponding to various sensors in fabrication equipment, with 104 failing observations total.

\newline
\pagebreak

# Data Wrangling

## Summary of Approach.

* Assemble csv files, update variable names and format as needed.
* Remove irrelevant variables.
* Classify all missing data as "NA".
* Impute missing data.

## Assemble Initial Dataset.

```{r raw data, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# Assemble Initial Dataset
#-----------------------------------------------------------------------
# Initial pass/fail data (Status = -1 => pass, 1 => fail).
path_in <- file.path("Data", "secom_labels.data")
secom_labels <- read.delim(path_in, header=F, sep=" ")
names(secom_labels) <- c("Status", "Date")
secom_labels <- separate(secom_labels, col="Date", into=c("Date", "Time"), sep=" ")
secom_labels$Status[which(secom_labels$Status == -1)] <- 0         # convert Status=-1 --> 0='pass', 1='fail')

# Sensor data.
path_in <- file.path("Data", "secom.data")
secom <- read.delim(path_in, header=F, sep=" ")                    # sensor data
secom <- cbind(secom_labels, secom)                                # P/F data

numVarsInit = round(length(names(secom)), 0)
numMissInit = round(n_miss(secom), 0)
```

The [SECOM Data Set](http://archive.ics.uci.edu/ml/datasets/secom) consists of two csv files. The first is a list of pass/fail and date/time results, one entry per wafer run, and the other contains the corresponding numeric results of sensor readings from a semiconductor manufacturing line. The data files do not contain a header so variable names 'Status', 'Date', and 'Time' were assigned for pass = -1 / fail = +1, date and time. The 'Status' variable is then updated to pass = 0 / fail = +1 for convenience when fitting the data to categorical models. The remaining variables assumed default names: V1, V2, etc. Table 2.1 below shows a sampling of the initial dataset. Since the sensor variables are not named in the original dataset, and since no information is provided about the physical meaning, source or process order of each sensor's data, there is no way to attribute any process or business meaning to the data. Therefore the analysis herein will take a "black box" approach. 

```{r wrangling table, echo=F, warning=F, message=F}
kable(secom[1:5,1:7], caption = 'Table 2.1: Sampling of the SECOM dataset.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

\newline
&nbsp;

## Initial Cleanup

```{r initial cleanup, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# Initial cleanup.
#-----------------------------------------------------------------------
# Remove variables with no distribution or < 1% unique values.
flagged <- c(NULL)                                                 # init. vector of flagged var. #s
for (n in 4:length(secom)) {
    if ((min(secom[n], na.rm=T) == max(secom[n], na.rm=T)) |
          (100*nrow(unique(secom[n]))/nrow(secom[n]) < varUnique)) {  # test for dist. and min. % of unique values
        print(paste("Dist. Filter: Variable ", names(secom[n])," dropped"))
        flagged <- c(flagged,-n)                                   # non-dist. variable columns
    }
}
secom_clean <- secom[flagged]                                      # new WIP dataset

# Consolodate all missing values to NA.
replace_with_na_all(data = secom_clean, condition = ~.x %in% c("N/A", "missing", "na", " "))

# Convert all NaN values to NA
secom_clean[is.na(secom_clean)] <- NA

#-----------------------------------------------------------------------
# Characterize extent of missing data.
#-----------------------------------------------------------------------
# Total missing values.
pctMissingInit <- round(100*n_miss(secom_clean)/(ncol(secom_clean)*nrow(secom_clean)), digits=2)
paste("Initially missing",pctMissingInit,"% of the total data.")

# Remove variables with more than 'maxMissing'%  observations.
secom_clean <- secom_clean[100*colSums(is.na(secom_clean))/nrow(secom_clean) < maxMissing]

pctMissing <- round(100*n_miss(secom_clean)/(ncol(secom_clean)*nrow(secom_clean)), digits=2)
paste("Need to impute remaining",pctMissing,"% of the missing total data.")

#-----------------------------------------------------------------------
# WIP dataset. 
#-----------------------------------------------------------------------
secom_wip <- secom_clean                                           # start & end each section with this df

```


```{r transform distributions, eval=T, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold'}
secom_trans <- secom_wip

# Once distributions are final, set transformFlag=F to read them from file.
if (transformFlag) {    # set flag in setup chunk

    # Clean Extreme Outliers
    secom_trans$V5[which(secom_trans$V5     >  900)] <- NA  # secom_trans$Status[which(secom_wip$V5 > 900)]
    secom_trans$V16[which(secom_trans$V16   >  800)] <- NA  # secom_trans$Status[which(secom_wip$V16 > 800)]
    secom_trans$V17[which(secom_trans$V17   >  100)] <- NA  # secom_trans$Status[which(secom_wip$V17 > 100)]
    secom_trans$V17[which(secom_trans$V18   <  0.7)] <- NA  # secom_trans$Status[which(secom_wip$V18 < 0.7)]
    secom_trans$V42[which(secom_trans$V42   >   30)] <- NA  # secom_trans$Status[which(secom_wip$V42 > 30)]
    secom_trans$V68[which(secom_trans$V68   >  750)] <- NA  # secom_trans$Status[which(secom_wip$V68 > 750)]
    secom_trans$V118[which(secom_trans$V118 >  300)] <- NA  # secom_trans$Status[which(secom_wip$V118 > 9000)]
    secom_trans$V141[which(secom_trans$V141 > 9000)] <- NA  # secom_trans$Status[which(secom_wip$V141 > 9000)]
    secom_trans$V148[which(secom_trans$V148 > 0.75)] <- NA  # secom_trans$Status[which(secom_wip$V148 > 0.75)]
    secom_trans$V149[which(secom_trans$V149 >  600)] <- NA  # secom_trans$Status[which(secom_wip$V149 > 600)]
    secom_trans$V153[which(secom_trans$V153 >  750)] <- NA  # secom_trans$Status[which(secom_wip$V153 > 750)]
    secom_trans$V155[which(secom_trans$V155 >  150)] <- NA  # secom_trans$Status[which(secom_wip$V155 > 150)]
    secom_trans$V186[which(secom_trans$V186 >  200)] <- NA  # secom_trans$Status[which(secom_wip$V186 > 200)]
    secom_trans$V188[which(secom_trans$V188 >  200)] <- NA  # secom_trans$Status[which(secom_wip$V188 > 200)]
    secom_trans$V205[which(secom_trans$V205 > 7500)] <- NA  # secom_trans$Status[which(secom_wip$V205 > 7500)]
    secom_trans$V224[which(secom_trans$V224 > 1500)] <- NA  # secom_trans$Status[which(secom_wip$V224 > 1500)]
    secom_trans$V253[which(secom_trans$V253 > 2000)] <- NA  # secom_trans$Status[which(secom_wip$V253 > 2000)]
    #secom_trans$V275[which(secom_trans$V275 >    1)] <- NA  # secom_trans$Status[which(secom_wip$V275 >    1)]
    secom_trans$V276[which(secom_trans$V276 > 3000)] <- NA  # secom_trans$Status[which(secom_wip$V276 > 3000)]
    secom_trans$V283[which(secom_trans$V283 >  0.3)] <- NA  # secom_trans$Status[which(secom_wip$V283 >  0.3)]
    secom_trans$V284[which(secom_trans$V284 >  200)] <- NA  # secom_trans$Status[which(secom_wip$V284 >  200)]
    secom_trans$V288[which(secom_trans$V288 >  250)] <- NA  # secom_trans$Status[which(secom_wip$V288 >  250)]
    secom_trans$V290[which(secom_trans$V290 >   50)] <- NA  # secom_trans$Status[which(secom_wip$V290 >   50)]
    secom_trans$V322[which(secom_trans$V322 >   60)] <- NA  # secom_trans$Status[which(secom_wip$V322 >   60)]
    secom_trans$V324[which(secom_trans$V324 >   75)] <- NA  # secom_trans$Status[which(secom_wip$V324 >   75)]
    secom_trans$V341[which(secom_trans$V341 > 2000)] <- NA  # secom_trans$Status[which(secom_wip$V341 > 2000)]
    secom_trans$V362[which(secom_trans$V362 >  400)] <- NA  # secom_trans$Status[which(secom_wip$V362 >  400)]
    secom_trans$V389[which(secom_trans$V389 >  250)] <- NA  # secom_trans$Status[which(secom_wip$V389 >  250)]
    secom_trans$V391[which(secom_trans$V391 >  700)] <- NA  # secom_trans$Status[which(secom_wip$V391 >  700)]
    secom_trans$V422[which(secom_trans$V422 >  200)] <- NA  # secom_trans$Status[which(secom_wip$V422 >  200)]
    secom_trans$V426[which(secom_trans$V426 >  750)] <- NA  # secom_trans$Status[which(secom_wip$V426 >  750)]
    secom_trans$V428[which(secom_trans$V428 >   90)] <- NA  # secom_trans$Status[which(secom_wip$V428 >   90)]
    secom_trans$V454[which(secom_trans$V454 >   30)] <- NA  # secom_trans$Status[which(secom_wip$V454 >   30)]
    secom_trans$V496[which(secom_trans$V496 >   90)] <- NA  # secom_trans$Status[which(secom_wip$V496 >   90)]
    secom_trans$V525[which(secom_trans$V525 >  700)] <- NA  # secom_trans$Status[which(secom_wip$V525 >  700)]
    secom_trans$V584[which(secom_trans$V584 >  0.4)] <- NA  # secom_trans$Status[which(secom_wip$V584 >  0.4)]
    secom_trans$V585[which(secom_trans$V585 >  0.1)] <- NA  # secom_trans$Status[which(secom_wip$V585 >  0.1)]
    secom_trans$V586[which(secom_trans$V586 >   80)] <- NA  # secom_trans$Status[which(secom_wip$V586 >   80)]
    
    # Translations
    translateVarList_1 <- c("V78","V80","V212","V218","V220","V251","V253","V278")
    secom_trans <- secom_trans %>% mutate_at(vars(translateVarList_1), ~(.+1))
    translateVarList_100 <- c("V60")
    secom_trans <- secom_trans %>% mutate_at(vars(translateVarList_1), ~(.+100))

    # log10() Transforms
    transformVarList_0 <- c("V1","V4","V8","V12","V13","V33","V34","V35","V36","V37","V40","V42","V61","V63","V64","V65","V66","V72","V90")
    transformVarList_1 <- c("V122","V125","V126","V127","V133","V136","V137","V138","V139","V140","V141","V143","V144","V145","V146","V147","V148","V151","V152","V153","V154","V155","V156","V157","V160","V161","V162","V163","V164","V165","V166","V167","V168","V169","V172","V185","V186","V189","V196","V197","V198","V199")
    transformVarList_2 <- c("V200","V201","V202","V203","V204","V205","V206","V208","V211","V213","V214","V223","V225","V226","V228","V229","V249","V250","V252","V254","V271","V274","V280","V281","V282","V283","V286","V287","V288","V289","V291","V292","V295","V296","V297","V298","V299")
    transformVarList_3 <- c("V300","V301","V302","V303","V304","V307","V321","V322","V324","V325","V332","V333","V334","V335","V336","V337","V338","V339","V340","V341","V342","V344","V349","V350","V351","V352","V353","V356","V358","V361","V363","V364","V366","V367","V368","V369","V387","V388","V390","V391","V392")
    transformVarList_4 <- c("V409","V412","V413","V414","V416","V417","V418","V419","V420","V421","V424","V425","V426","V427","V429","V430","V431","V432","V433","V434","V435","V436","V437","V438","V439","V440","V443","V455","V457","V458","V460","V461","V468","V469","V470","V471","V472","V474","V475","V476","V477","V478","V480","V481","V483","V484","V485","V486","V487","V488","V489","V490","V491","V492","V495","V497")
    transformVarList_5 <- c("V511","V521","V523","V524","V525","V526","V541","V559","V560","V561","V562","V571","V573","V574","V575","V577","V578","V584","V585","V586","V588","V589","V590")
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_0), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_1), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_2), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_3), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_4), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(transformVarList_5), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(translateVarList_1), funs(log10))
    secom_trans <- secom_trans %>% mutate_at(vars(translateVarList_100), funs(log10))

    # Convert any remaining missing values to 'NA'.
    secom_trans[is.na(secom_trans)] <- NA                                  # Convert any NaN values to NA
    secom_trans <- do.call(data.frame, lapply(secom_trans, function(x) {
        replace(x, is.infinite(x), NA)                                     # Convert any Inf values to NA
        })
    ) #end do.call

    # Scale data
    secom_scaled <- scale(as.matrix(secom_trans[,which(grepl("^V",names(secom_trans)))]))
    secom_scaled <- bind_cols(secom_trans['Status'], secom_trans['Date'], secom_trans['Time'], data.frame(secom_scaled))
    
    # Save transformed datasets for later use.
    path_out <- file.path("Data", "secom_trans.csv")                       # saved for later use
    write.csv(secom_trans, path_out)
    path_out <- file.path("Data", "secom_scaled.csv")                      # WIP dataset
    write.csv(secom_scaled, path_out)
} else {
    path_in <- file.path("Data", "secom_trans.csv")
    secom_trans <- read.csv(path_in, stringsAsFactors=F) 
    secom_trans <- within(secom_trans, rm(X))                              # drop extra variable    
    path_in <- file.path("Data", "secom_scaled.csv")
    secom_scaled <- read.csv(path_in, stringsAsFactors=F) 
    secom_scaled <- within(secom_scaled, rm(X))                            # drop extra variable    
} #end if (transformFlag)

# Current missings status.
pctMissing <- round(100*n_miss(secom_scaled)/(ncol(secom_scaled)*nrow(secom_scaled)), digits=2)
paste("Need to impute remaining",pctMissing,"% of the missing total data.")

# Density + Q-Q Plots
plot_data <- secom_scaled
sensorVars <- names(plot_data)[grepl("^V",names(plot_data)) & !grepl("_NA$",names(plot_data))]
#distMatrix(plot_data, plotVars=sensorVars, plotDim=3, nStart=1, nPlot=9, nBins=100, QQplot=TRUE)
#distMatrix(plot_data, plotVars=sensorVars, plotDim=3, nStart=1, nBins=100, QQplot=F)

# Pass WIP dataset to next step.
secom_wip <- secom_scaled
```


Sensor data comprise real valued random variables by nature. So any variable that contains only missing data or has no variation is irrelevant for this analysis and can be dropped. The approach taken here was to drop all variables where the distribution min = max. It's not clear why these data were included in the SECOM dataset, but since the goal is to identify signals or combinations of signals leading to an alarm condition, unvarying sensor data are irrelevant. The next important issue with the dataset was to properly classify all missings as "NA". Missing results can be defined by a number of non-standard labels including "N/A", "missing", "na" or even " ". The [**naniar package**](https://cran.r-project.org/web/packages/naniar/index.html) provides a simple function replace_with_na_all() to simplify converting this arbitrary list of labels to "NA". Finally, there were a number of "NaN" designations that aren't typically interpreted as missings, but since the sensor data should be real values it was determined that these values should be treated as "NA". It was found that initially `r pctMissingInit`% of the dataset was missing. While that doesn't seem to be too significantly large, it depends on how missingness is distributed within the dataset. Among the many useful features of the naniar package are plotting routines for visually exploring missingness. One of the routines gg_miss_var() is shown below in Fig. 2.1 in which the variables are ordered by total missingness and plotted on the y-axis, and the number of missing observations on the x-axis. The number variables in this dataset is too large for printing so are omitted from the y-axis. The notable takeaway here was that most of the missing data was limited to a relatively few number of variables. The safe approach taken was to drop all variables with > `r maxMissing`% missing data leaving just `r pctMissing`% total missing data for imputation.


\newline
&nbsp;

```{r initial missings visual, echo=F, warning=F, message=F, results='hide', out.width='90%', fig.align='center', fig.cap='Fig. 2.1: Visualization of initial missingness.'}
#-----------------------------------------------------------------------
# Visualize extent of missing data.
#-----------------------------------------------------------------------
# naniar visualize initial missing data.
gg_miss_var(secom) +
  theme(axis.text.y = element_text(color = "white", size = 2)) +
  labs(title = "Summy of Missing Data in SECOM Dataset",
       y = "# Missing Observations",
       x = "Variables")
#vis_miss(secom_clean, cluster = T, sort_miss = T)
#gg_miss_which(secom_clean)
#gg_miss_fct(secom_clean, fct=Status)
```

\newline
&nbsp;

## Imputation

```{r imputation, eval=T, echo=F, message=F, warning=F, results='hide'}

# Scaled data
#secom_scaled <- scale(as.matrix(secom_wip[,which(grepl("^V",names(secom_wip)))]))
#secom_scaled <- bind_cols(secom_wip['Status'], secom_wip['Date'], secom_wip['Time'], data.frame(secom_scaled))
#secom_wip <- secom_scaled

#-----------------------------------------------------------------------
# Imputation: MICE
#-----------------------------------------------------------------------
dataSet <- secom_wip  #[1:50]  # Useful for limiting data for debug.
secom_shadow <- dataSet %>%
  bind_shadow() %>%
  add_label_shadow() 

# Extract naniar shadow matrix.
secom_shadow <- secom_shadow[grepl("_NA$",names(secom_shadow)) |  grepl("any_missing",names(secom_shadow))]

# Once imputed values are final set imputeFlag=F to read them from file.
if (imputeFlag) {    # set flag in setup chunk
    # Update prediction matrix.
    ini <- mice(dataSet, maxit=0, print=F)
    pred <- ini$pred
    pred[,c("Status","Date","Time")] <- 0                                   # vars not used as predictors

    secom_imp <- dataSet %>%
      mice(m=1, method='cart',      pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method='rf',      pred=pred, maxit = 250, seed=100) %>%
      #mice(m=1, method="norm",         pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="norm.nob",      pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="norm.boot",      pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="norm.predict", pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="quadratic",     pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="gamlssTF",     pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="2l.norm",         pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="2l.lmer",         pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="2l.pan",         pred=pred, maxit = 1, seed=100) %>%
      #mice(m=1, method="2lonly.norm",         pred=pred, maxit = 1, seed=100) %>%
      complete(1) 
    
    # Save imputed dataset for later use.
    path_out <- file.path("Data", "secom_imp.csv")
    write.csv(secom_imp, path_out)
} else {
    path_in <- file.path("Data", "secom_imp.csv")
    secom_imp <- read.csv(path_in, stringsAsFactors=F) 
    secom_imp <- within(secom_imp, rm(X))                          # drop extra variable    
    #secom_imp$Status <- as.factor(secom_imp$Status)
} #end if (imputeFlag)

paste("There are", n_miss(secom_imp), "missing values remaining.")
pctMissing <- round(100*n_miss(secom_imp)/(ncol(secom_imp)*nrow(secom_imp)), digits=2)
paste("Need to impute remaining",pctMissing,"% of the missing total data.")

# Merge data with shadow matrix.
secom_imp_bound <- cbind(secom_imp, secom_shadow)
dataSet_bound <- cbind(dataSet, secom_shadow)
bound_data <- bind_rows(original = dataSet_bound,
                        imputed = secom_imp_bound,
                        .id = "data_type")

# Final Stats
numVars = length(names(secom_imp))
numMiss = n_miss(secom_imp)

#-----------------------------------------------------------------------
# WIP dataset. 
#-----------------------------------------------------------------------
secom_wip <- secom_imp                                           # start & end each section with this df

```

There are several R packages for imputing data. Initially, the [simputation](https://cran.r-project.org/web/packages/simputation/index.html) package was chosen for it's ease of use and integration with naniar and ggplot2. Unfortunately, the number of variables in this dataset created multiple run-time issues for the simputation engine so it had to be abandoned. Instead, the [mice package ](https://cran.r-project.org/web/packages/mice/index.html), which stands for "Multivariate Imputation by Chained Equations", provided powerful fitting functionality at a moderate computation cost. The package is capable of fitting a different imputation model to each variable, but the norm.nob method was applied unilaterally and found to return reasonably good values on comparing pre- and post-imputation distributions. Fig. 2.2 shows a sample distribution for two model features overlaying the imputed values in the histogram. Fig. 2.3 is a scatterplot of these two features demonstrating how pairwise distribution relationships are maintained by imputed data. A summary of the data wrangling and imputation effort is shown in Table 2.2. Of the initial 593 total variables in the dataset, the post-wrangling count was `r length(names(secom_wip)[grepl("^V",names(secom_wip)) & !grepl("_NA$",names(secom_wip))])`, and the initial 4.51% total missing data has been reduced to zero.

\newline
&nbsp;

```{r sample imputed distribution, eval=T, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 2.2: Example distributions before and after imputation'}
# Review results.
sensorVars <- names(secom_wip)[grepl("^V",names(secom_wip)) & !grepl("_NA$",names(secom_wip))]
distMatrix(bound_data, plotVars=sensorVars, factorBy="data_type", plotDim=1, nStart=1, nPlot=2, nBins=200, numXaxes=1)
```

```{r sample imputed scatter, eval=T, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 2.3: Example scatterplot overlay of imputed and original data. Imputed data is shown standalone on the right.'}
# Review results.
sensorVars <- names(secom_wip)[grepl("^V",names(secom_wip)) & !grepl("_NA$",names(secom_wip))]
p <- secom_imp_bound %>% dplyr::select(V1,V2,V1_NA,V2_NA)
p$data_type <- "original"
p$data_type[which(p$V1_NA == "NA" | p$V2_NA == "NA")] <- "imputed"
p %>% 
  ggplot(aes(x=V1, y=V2, color=as.factor(data_type), fill=as.factor(data_type))) +
    geom_point(size=3, alpha=0.4) +
    scale_color_brewer(palette="Set1", type="div") +    # colors
    scale_fill_brewer(palette="Set1", type="div") +     # fillxlim(-4.5,4.5) +
    ylim(-4.5,4.5)
p %>% filter(data_type == "imputed") %>%
  ggplot(aes(x=V1, y=V2, color=as.factor(data_type))) +
    geom_point(size=3, alpha=0.6) +
    scale_color_brewer(palette="Set1", type="div") +    # colors
    scale_fill_brewer(palette="Set1", type="div") +     # fillxlim(-4.5,4.5) +
    xlim(-4.5,4.5) +
    ylim(-4.5,4.5)

```

\newline
&nbsp;

```{r wrangling summary table, eval=T, echo=F, warning=F, message=F, out.width='40%'}

Metric <- c("# of Variables", "# of Observations", "% Missings")
Initial <- c(round(as.integer(numVarsInit),0), round(as.integer(nrow(secom)),0), round(100*numMissInit/(numVarsInit*nrow(secom)), 2))
Final <- c(as.integer(numVars), as.integer(nrow(secom_wip)), round(100*numMiss/(numVars*nrow(secom_wip)), 2))
wrangled <- data.frame(Metric, Initial, Final)

kable(wrangled, caption = 'Table 2.2: Summary of data wrangling.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

\newline
&nbsp;

# Data Exploration

## Sensor Data Distributions

An important requirement for developing a multivariate model for this project is how well the individual sensor data distributions can be modeled by a known statistical distribution type. Ideally each variable would follow the well known normal distribution. The figure below shows an example of one variable in the SECOM dataset that is roughly normal based on visual inspection of the probability density (PDF) distribution. Included below that is its corresponding Q-Q plot, or quantile plot, which plots the measured versus theoretical quantile data. For an ideal normal distribution the fit line of a Q-Q plot would be colinear with the data and have very small residuals over the entire +3/-3 z-score range. The farther the fit line deviates from data the less confident we can be that the distribution is normal. A reasonable target is that at least 95% of the data fits the distribution which corresponds to a good fit between z = +/-1.96. Since the goal of the model in this project is to predict physical wafer failures based on production sensor data, the PDF and Q-Q plots are also shown comparing passing and failing results. Except for differences in the tails, there's very little distinction between the distributions. Since it's not known if the differences in the tail data is important or not, the apparent outliers will not be removed from the analysis yet. The second set of plots in Fig. 3.2 below show the data for another sensor which also follows a normal distribution, but comparison of yield results shows differences in both the peak probability and tail distribution. 

\newline
&nbsp;

```{r plot Var2 density, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.1: Example of near normal distributions.'}
# Copy of data for plotting.
#plot_data <- secom_wip
plot_data <- secom
sensorVars <- names(plot_data)[grepl("^V",names(plot_data)) & !grepl("_NA$",names(plot_data))]
plot_data$Yield <- as.factor(plot_data$Status)
levels(plot_data$Yield) <- c("pass", "fail")

#Density Plots
distMatrix(plot_data, plotVars="V2",                   plotDim=1, nStart=1, nPlot=1, nBins=100, title="Density Plot for All 'V2' Data")
distMatrix(plot_data, plotVars="V2", factorBy='Yield', plotDim=1, nStart=1, nPlot=1, nBins=100, title="Density Plot for 'V2' by Yield")

# Q-Q Plots
ggplot(plot_data, aes(sample=V2)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for All 'V2' Data")

ggplot(plot_data, aes(sample=V2, color=Yield)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5) +
  ggtitle("Q-Q Plot for 'V2' by Yield")
```

\newline
&nbsp;

```{r plot Var3 density, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.2: Example of near normal distributions with yield differences.'}

# Density Plots
distMatrix(plot_data, plotVars="V3",                   plotDim=1, nStart=1, nPlot=1, nBins=100, 
           title="Density Plot for All 'V3' Data")
distMatrix(plot_data, plotVars="V3", factorBy='Yield', plotDim=1, nStart=1, nPlot=1, nBins=100, 
           title="Density Plot for 'V3' by Yield")

# Q-Q Plots
ggplot(plot_data, aes(sample=V3)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for All 'V3' Data")

ggplot(plot_data, aes(sample=V3, color=Yield)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5) +
  ggtitle("Q-Q Plot for 'V3' by Yield")
```


\newline
&nbsp;

What happens then for data that do not exhibit a normal distribution? The figure below shows the case for a right-skewed distribution at the top left with corresponding Q-Q plot below it. The longer right tail is clear in the PDF but really stands out in the Q-Q plot where the trend sharply deviates near z = +1. For distributions like this one the plan is to transform the original data into a form that is closer to normal. The plots on the right side show the result after taking the logarithm of the sensor values. The effect is seen as rebalancing the distribution, making it more symmetric about the mean. There are many non-normal distribution types available for custom fitting these data, but the approach that will be taken here will be to apply logarithmic transform, or shift + transform. The problematic situation is when the data do not follow any single distribution type but is comprised of a superposition of two or more component distributions. Fig. 3.4 below shows examples of multi-modal sensor distributions. This data could be fit with a superposition of distributions, but review of the Q-Q plots show that a normal distribution can describe the overall distribution adequately. In this project then, multi-modal distributions will be approximated by a normal or log-normal distribution as best fits the data.

\newline
&nbsp;

```{r plot log density, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.3: Example skewed distribution.'}

# Density Plots (linear and log scales)
distMatrix(plot_data, plotVars="V65", plotDim=1, nStart=1, nPlot=1, nBins=100, numXaxes = 2)

# Q-Q Plot
ggplot(plot_data, aes(sample=V65)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for 'V65'")
# Q-Q Plot
ggplot(plot_data, aes(sample=log(V65))) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for All log('V65')")
```

\newline
&nbsp;

```{r plot multi-mode density, echo=F, warning=F, message=F, results='hide', out.width='50%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.4: Example multi-modal distributions.'}

distMatrix(plot_data, plotVars="V22", factorBy='Yield', plotDim=1, nStart=1, nPlot=1, nBins=100,
           title="Bi-Modal Distribution")
distMatrix(plot_data, plotVars="V52", factorBy='Yield', plotDim=1, nStart=1, nPlot=1, nBins=100,
           title="tri-Modal Distribution")

# Q-Q Plot
ggplot(plot_data, aes(sample=V22)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for 'V22'")
# Q-Q Plot
ggplot(plot_data, aes(sample=V52)) +
  geom_qq(size=2) +
  geom_qq_line(size=1.5, color="red") +
  ggtitle("Q-Q Plot for 'V52'")

```

\newline
&nbsp;


## Correlation Analysis

A scatter plot is good way to assess relationships between pairs of variables. The more that the data trends with a positive slope the more the two variables are correlated. On the other hand, the more the data trends with a negative slope the more the two variables are anti-correlated. When building a model with a few variables this is a convenient way to visually identify the relevant features. The example scatter plot matrix below shows the relationships between the first ten variables in the SECOM dataset plus the dependent variable 'Status'. To quantify the relationship between two variables a correlation analysis is run and generates a correlation coefficient between -1 and +1. Correlated data obtain a coefficient > 0 up to a maximum of 1 and anti-correlated data obtain a coefficient < 0 down to a minimum of -1. Fig 3.6 shows examples of highly correlated (top), highly anti-correlated (bottom) and uncorrelated (middle) variable pairs in the SECOM dataset. Table 3.1 below that lists the same information in tabular format.

\newline
&nbsp;

```{r scatter plot example, echo=F, message=F, warning=F, results='hide', out.width='90%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.5: Example scatter plot matrix.'}
#-----------------------------------------------------------------------
# Scatter Plot Matrix
#-----------------------------------------------------------------------
dataSet <- secom_wip[1:12]                                       # Useful for limiting data for debug.
dataSet[c('Date','Time','Yield')] <- c(NULL)

plot(dataSet)
```

\newline
&nbsp;

```{r sample correlation matrix, echo=F, message=F, warning=F, results='hide', out.width='90%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.6: Sample correlation coefficient ranges for the SECOM dataset.'}

#-----------------------------------------------------------------------
# Sample Correlation Matrix
#-----------------------------------------------------------------------
dataSet <- secom_wip
dataSet[c('Date','Time','Yield')] <- c(NULL)                                         # drop variables
corr_df <- rcorr(as.matrix(dataSet))                                                 # Hmisc correlation analysis
corr_list <- flattenCorrMatrix(corr_df$r, corr_df$P)                                 # pairwise variable coeff list
corr_list <- mutate(corr_list, pairName=paste(var1,"-",var2,sep=""))                 # unite var1 and var2 names
botCorr <- arrange(corr_list, coeff)[1:10,]                                          # lowest correlation coeff.
topCorr <- arrange(corr_list, desc(coeff))[1:10,]                                    # highest correlation coeff.
midCorr <- corr_list[(corr_list$coeff >= -0.000005) & (corr_list$coeff <= 0.000005),]# mid-range correlation coeff.
corr_sample <- arrange(bind_rows(bottom = botCorr,
                                 middle = midCorr,
                                 top    = topCorr, 
                                 .id = "Range"), coeff)

# correlation coeff vs. variable parings
ggplot(corr_sample, aes(reorder(pairName,coeff), coeff, col=Range)) +
  geom_point(size=2) +
  theme(axis.text.x = element_text(size = 10, angle = 60, hjust=1)) +
  labs(title = "Sample of Correlation Coefficents",
       x = "Variable Pair Names",
       y = "Correlation Coefficient")

# p-values vs. variable parings
#ggplot(corr_sample, aes(reorder(pairName,coeff), p, col=Range)) +
#  geom_point(size=2) +
#  theme(axis.text.x = element_text(size = 10, angle = 60, hjust=1)) +
#  labs(title = "Sample of Correlation Coefficents",
#       x = "Variable Pair Names",
#       y = "P-Values")
```

\newline
&nbsp;

```{r sample correlation table, echo=F, warning=F, message=F, out.width='50%'}
#corr_sample$Range <- NULL
corr_sample$pairName <- NULL
corr_sample$p <- NULL
corr_sample$coeff <- round(corr_sample$coeff,4)
kable(corr_sample, caption = 'Table 3.1: Data used for Fig. 3.6.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

\newline
&nbsp;

A correlogram is another useful visualization tool for spotting trends in variable relationships. The correlogram reduces the information in the scatter plot matrix to a color-coded matrix of correlation coefficients for easy identification of trends. The relationships for the first 10 variable plus the dependent variable are shown below in Fig. 3.7. In a dataset with hundreds of variables these visual aids aren't very useful. Fig. 3.8 below shows the case for the current SECOM dataset. The plot does show interesting clustering trends but this amount of data is too cumbersome to perform visual analyses. The approach that will be taken in this project is to select features based on their statistical significance in a given model.

\newline
&nbsp;

```{r correlogram example, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.cap='Fig. 3.7: Example correlogram plot.'}
#-----------------------------------------------------------------------
# Sample Correlogram
#-----------------------------------------------------------------------
dataSet <- secom_wip[1:12]                                       # Useful for limiting data for debug.
dataSet[c('Date','Time','Yield')] <- c(NULL)

corr_data <- cor(dataSet)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot::corrplot(corr_data, method="color", col=col(200), type="upper", order="original", diag=FALSE, 
         addCoef.col = "black", 
         tl.cex=0.75, tl.col="black", tl.srt=45) 

```

\newline
&nbsp;

```{r correlogram, echo=F, message=F, warning=F, results='hide', out.width='90%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.8: Correlogram plot for the SECOM dataset.'}
#-----------------------------------------------------------------------
# Correlogram
#-----------------------------------------------------------------------
dataSet <- secom_wip #[1:50]                                       # Useful for limiting data for debug.
dataSet[c('Date','Time','Yield')] <- c(NULL)

corr_data <- cor(dataSet)

# correlogram with hclust reordering
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot::corrplot(corr_data, method="color", col=col(200), type="upper", order="original", diag=FALSE, 
         tl.cex=0.05, tl.col="black", tl.srt=45) 

```

\newline
&nbsp;

While the number of cross-correlations makes visual analysis cumbersome in general, it is still interesting to review how each variable is correlated to the dependent variable, 'Status'. Fig. 3.9 plots the SECOM variables ordered by their correlation coefficient with 'Status'. The notable takeaway is that none of the individual variables is significantly correlated with the output response meaning that the final model should expect to retain a significant number of independent variables. 

\newline
&nbsp;

```{r dependent variable correlations, echo=F, message=F, warning=F, results='hide', out.width='90%', fig.align='center', fig.show='hold', fig.cap='Fig. 3.9: Correlation coefficients for the dependent variable, Status.'}

#-----------------------------------------------------------------------
# Dependent Variable Correlations
#-----------------------------------------------------------------------
corr_dv <- arrange(corr_list[which(corr_list$var1 == "Status"),], coeff)

ggplot(corr_dv, aes(reorder(var2,coeff), coeff)) +
  geom_point(size=0.5) +
  theme(axis.text.x  = element_text(size = 4, angle = 60, vjust = grid::unit(c(0.2, -0.1, -0.4), "points")),
        axis.title.x = element_text(vjust = 30)) +
  
  labs(title = "Correlation Coefficents for Dependent Variable 'Status'",
       x = "Variable Names",
       y = "Correlation Coefficient")
```

\newline
&nbsp;


# Modeling

## Summary of Approach.

* 3-part data split to train/tune/test (85%/10%/5%) model fits.
* Baseline logistic regression model fit.
* Iterative logistic regression model fit based on fit coefficient significance.
* Balance primary pass/fail variable (Status) classes using SMOTE.
* Repeat iterative logistic regression model fit.
* Random Forest classification model fit.

## Data Splitting

The data approach taken here was to split the available data into 3 parts for initial model training (85%), model tuning (10%) and final test verification (5%). Table 4.1 below shows the breakdown of available data by number of observations and percentage of the total available dataset, and the number of pass/fail observations for the datasets used here. The greater than 10:1 pass:fail delta in these datasets is notable and addressed further into the analysis. 

```{r split dataset, echo=F, message=F, warning=F, results='show', out.width='50%'}
#-----------------------------------------------------------------------
# Split Training and Test Sets
#-----------------------------------------------------------------------
# 2-Part Split
#split <- sample.split(secom_wip$Status, SplitRatio = 0.75)
#secom_train <- secom_wip[which(split == TRUE),]
#secom_tune <- secom_wip[which(split == FALSE),]

# 3-Part Split
split1 <- sample.split(secom_wip$Status, SplitRatio = 0.85)
secom_train <- secom_wip[which(split1 == TRUE),]
temp_test <- secom_wip[which(split1 == FALSE),]
split2 <- sample.split(temp_test$Status, SplitRatio = 2/3)
secom_tune <- temp_test[which(split2 == TRUE),]
secom_verify <- temp_test[which(split2 == FALSE),]

# Summary Table
secom_table <- table(secom_wip$Status)
train_table <- table(secom_train$Status)
tune_table <- table(secom_tune$Status)
test_table <- table(secom_verify$Status)
secom_state <- c(nrow(secom_wip),round(100,2),as.integer(secom_table[1]),as.integer(secom_table[2]))
train_state <- c(nrow(secom_train),round(100*nrow(secom_train)/nrow(secom_wip),2),as.integer(train_table[1]),as.integer(train_table[2]))
tune_state <- c(nrow(secom_tune),round(100*nrow(secom_tune)/nrow(secom_wip),2),as.integer(tune_table[1]),as.integer(tune_table[2]))
test_state <- c(nrow(secom_verify),round(100*nrow(secom_verify)/nrow(secom_wip),2),as.integer(test_table[1]),as.integer(test_table[2]))
status_state <- as.data.frame(rbind(secom_state,train_state,tune_state,test_state))
colnames(status_state) <- c("Observations","Percent","Pass","Fail")
rownames(status_state) <- c("All Data","Train","Tune","Test")

kable(status_state, caption = 'Table 4.1: Data split and Status class totals.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

## Baseline Logistic Regression

### Training 

```{r baseline logistic model train, echo=F, message=F, warning=F}
#-----------------------------------------------------------------------
# Baseline Model: bayesglm
#-----------------------------------------------------------------------
trainData <- secom_train 
trainData[c('Date','Time')] <- c(NULL)

# Baseline glm model fit.
fit_glm_baseline_1 <- bayesglm(Status ~ . , data=trainData, family="binomial", maxit=200) # baseline glm fit model
Status_pred <- fitted(fit_glm_baseline_1)                          # predicted model vals
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Other useful functions 
summary <- summary(fit_glm_baseline_1)                               # summary results
AIC <- round(as.double(summary[5],1))
#coefficients(fit_lm_baseline_1)                                     # model coefficients
#confint(fit_lm_baseline_1, level=0.95)                              # CIs for model parameters 
#fitted(fit_lm_baseline_1)                                           # predicted values
#residuals(fit_lm_baseline_1)                                        # residuals
#anova(fit_lm_baseline_1)                                            # anova table 
#vcov(fit_lm_baseline_1)                                             # covariance matrix for model parameters 
#influence(fit_lm_baseline_1)                                        # regression diagnostics

# Confusion Matrix
cm <-caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(trainData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "logistic"
stage <- "train"
stats_glm_train <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

For this initial baseline model all available features are used for a logistic regression. The glm model was unable to converge consistently so the [**bayesglm**](https://cran.r-project.org/web/packages/arm/index.html) logistic model was chosen instead. The bayesglm model uses the Student-t distribution instead of the normal distribution which better describes the SECOM variable distributions in this work. Due to the number of available features the fitting results are lengthy so are shown in Appendix B. The Akaike Information Criterion (AIC) is used as an estimator of the relative fit quality when comparing model fits for a given dataset. The criteria quantifies the model quality by weighing the goodness of fit with the number of estimators used. The few estimators needed for a given fit, the lower the relative number and the better the model. AIC says nothing about the absolute quality of a model, but the model's low AIC score of `r AIC` does indicate a parsimonious model. The confusion matrix results below show the fit accuracy = `r ACC`, sensitivity (TPR) = `r TPR` and selectivity (TNR) = `r TNR` with an F1 score = `r F1`. Since the purpose of the model is to accurately predict failures, TPR is a primary measure of the model capability. Accuracy and F1 score follow in assessing the overall model performance. Overall the base training model is very good.

```{r baseline logistic model train confusion matrix, echo=F, message=F, warning=F}
cm
```

```{r baseline logistic model initial test, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_tune
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_1, 
                       newdata = testData, 
                       type = "response")                          # predicted model 
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Confusion Matrix
cm <-caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

```

After running the tuning data against the model the accuracy drops to `r ACC`, TPR to `r TPR` and F1 score to `r F1`. Overall this is okay but TNR is very low. 

```{r baseline logistic model init test confusion matrix, echo=F, message=F, warning=F}
cm
```

### ROC Analysis

```{r baseline logistic model ROC, eval=T, echo=F, message=F, warning=F, results='hide'}

tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_1, 
                       newdata = tuneData, 
                       type = "response")                          # predicted model 

ROCRpred = ROCR::prediction(Status_pred, tuneData$Status)
ROCRperf = ROCR::performance(ROCRpred, measure="tpr", x.measure="fpr")
AUC <- round(as.numeric(ROCR::performance(ROCRpred, "auc")@y.values),4)
#print(paste("AUC = ",AUC,sep=""))

```

The area under the receiver operating characteristic (ROC) curve, or area under the curve (AUC), illustrates the diagnostic ability of a binary classifier model as its discrimination threshold is varied between 0 and 1. The curve is a plot of the true positive rate (TPR, sensitivity) against the false positive rate (1 - TNR, 1 - specificity) with the range of AUC between 0.5 and 1. A score of 0.5 corresponds to having little discernability or little predictability better than chance and is represented by a 45 degree line. A score of 1 corresponds to perfect discernability and is represented by a curve that tents to the upper left-hand corner. For this model AUC = `r AUC`. The ROC curve for the tuning dataset is plotted below. 


```{r baseline logistic model ROC plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.1: Receiver operating characteristic (ROC) curve for baseline logistic model fit.'}

plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1))
```

### Threshold Tuning

```{r baseline logistic model tune threshold, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.2: TPR, TNR, accuracy and F1 score vs. threshold.'}

threshold_glm_baseline <- tune_threshold(data = Status_pred, reference = tuneData$Status)

```


```{r baseline logistic model ROC tune, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_1, 
                       newdata = tuneData, 
                       type = "response")                          # predicted model 
Status_pred[Status_pred < threshold_glm_baseline] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_baseline] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(tuneData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "logistic"
stage <- "tune"
stats_glm_tune <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Fig. 4.2 plots the model performance versus threshold. Based on the criteria for high TNR and high accuracy, the intersection of these two curves is chosen as the optimum threshold. Setting the threshold to `r threshold_glm_baseline` increases TNR to `r TNR`, but decreases accuracy to `r ACC` and TPR to `r TPR`. F1 score = `r F1`.

```{r baseline logistic model ROC tune confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Test

```{r baseline logistic model verify, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_verify
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_1, 
                       newdata = testData, 
                       type = "response")                          # predicted model 
Status_pred[Status_pred < threshold_glm_baseline] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_baseline] <- 1                         # reset "fail" level

# Confusion Matrix
cm <-caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "logistic"
stage <- "test"
stats_glm_test <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Test data results below show accuracy = `r ACC`, TPR = `r TPR` and TNR = `r TNR`. F1 score = `r F1`.

```{r baseline logistic model verify confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Summary

```{r baseline logistic model summary, eval=T, echo=F, message=F, warning=F}

model_glm <- rbind(stats_glm_train, stats_glm_tune, stats_glm_test, deparse.level=1, make.row.names=F)
colnames(model_glm) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_glm, caption = 'Table 4.2: Logistic model summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```


```{r baseline logistic model parameter summary plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.3: TPR, TNR, accuracy and F1 score vs. model fit stage'}

myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
                   brewer.pal(9, "Reds")[c(3,5,6,8,9)])

model_glm %>%  
    gather(key="var", value="value", c(3:6)) %>%
    ggplot(aes(factor(Stage, level=c("train","tune","test")), value, group=var, col=var)) +
      geom_line(size=1) +
      geom_point(size=2, aes(color=var)) +
      scale_color_manual(values = myCol) +
      labs(title = "Baseline Logistic Model Performance at Each Stage",
           x = "Model Stages")

```


## Iterative Logistic Regression

### Training

```{r iterative baseline logistic model, eval=T, echo=F, message=F, warning=F}
#-----------------------------------------------------------------------
# Logistic Model (iterative coefficient optimization) : bayesglm
#-----------------------------------------------------------------------
trainData <- secom_train 
trainData[c('Date','Time','Yield')] <- c(NULL)

# Initialize loop variables.
loopCount <- 1                                                         # loop counter
maxPval <- 1                                                           # intial max. p-value
fitStats <- data.frame(loopCount=0,Fstat=0,df=0,Rsq=0)                 # loop stat dataframe
pathname <- "Data/glm_regression_models"                               # base path to glm model files
modelFile <- "fit_glm_baseline_2.rda"
fitConstFile <- "glm_fit_constants.csv"

if(file.exists(file.path(pathname, modelFile))) {
  
    load(file.path(pathname, modelFile))                               # load model
    c <- read.csv(file.path(pathname, fitConstFile), stringsAsFactors=F) # load fit constants
    c <- within(c, rm(X))                                              # drop extra variable    
  
} else {                                                               # else refit model
  
    while (maxPval > maxSig) {
        fit_glm_baseline_2 <- bayesglm(Status ~ . , data=trainData, family="binomial", maxit=200)   # baseline glm fit model
        c <- data.frame(coef(summary(fit_glm_baseline_2)))
        vars <- rownames(c)
        c <- bind_cols(data.frame(vars, stringsAsFactors=F), c)
        c <- c[c(-1),]                                                 # Remove intercept.
        
        df <- summary(fit_glm_baseline_2)$df[1]                        # degrees of freedom
        fitStats[loopCount,] <- c(loopCount, df)
        
        maxPval <- max(abs(c$Pr...z..))                                # current max p-value
        dropVar <- c$vars[which(c(abs(c$Pr...z..) == maxPval))]        # var name with max p-value
        trainData[dropVar] <- NULL
        
        print(paste("Loop#:",loopCount, "  Var. Count =", length(trainData)))
        maxPval <- max(abs(c$Pr...z..))                                # current p-value
        loopCount <- loopCount + 1
    } #end while
    
    save(fit_glm_baseline_2, file=file.path(pathname, modelFile))
    write.csv(c, file.path(pathname, fitConstFile))
    
} # end if file.exists

Status_pred <- fitted(fit_glm_baseline_2)                          # predicted model vals
#Status_pred <- predict(fit_glm_baseline_2)                          # predicted model vals
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Other useful functions 
summary <- summary(fit_glm_baseline_2)                               # summary results
AIC <- round(as.double(summary[5],1))
#coefficients(fit_glm_baseline_2)                                  # model coefficients
#confint(fit_glm_baseline_2, level=0.95)                           # CIs for model parameters 
#fitted(fit_glm_baseline_2)                                        # predicted values
#residuals(fit_glm_baseline_2)                                     # residuals
#anova(fit_glm_baseline_2)                                         # anova table 
#vcov(fit_glm_baseline_2)                                          # covariance matrix for model parameters 
#influence(fit_glm_baseline_2)                                     # regression diagnostics

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(trainData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic"
stage <- "train"
stats_iterglm_train <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

In the original logistic fit many coefficients showed very little significance, meaning their significance values were > 0.05. In the following fitting approach the logistic regression is performed iteratively with the least significant variable removed after each iteration. This assures that only the most significant features remain. Please refer to Appendix B for model output details. This approach shows a significant improvement in AIC score of `r AIC`. The accuracy = `r ACC`, F1 score = `r F1` and TPR = `r TPR` are also very good, but TNR = `r TNR` has dropped significantly compared with the base model. Confusion matrix results and a plot of most to least significant model features are shown below.

```{r iterative baseline logistic model confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```


```{r iterative baseline logistic model plots, echo=F, warning=F, message=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.4: Significant model features after class balancing, ordered most to least signicant from left to right.'}

#-----------------------------------------------------------------------
# Significant Model Features 
#-----------------------------------------------------------------------
#ggplot(c, aes(reorder(vars,Std..Error), Std..Error)) +
#  geom_point(size=2) +
#  scale_y_log10() +
#  theme(axis.text.x  = element_text(size = 8, angle = 60, vjust = 0.5)) +
#  labs(title = "Significant Model Features",
#       x = "Variable Names",
#       y = "Std. Error")

ggplot(c, aes(reorder(vars,Pr...z..), Pr...z..)) +
  geom_point(size=2) +
  scale_y_log10() +
  theme(axis.text.x  = element_text(size = 8, angle = 60, vjust = 0.5)) +
  labs(title = "Significant Model Features",
       x = "Variable Names",
       y = "Probability")

```


```{r baseline iterated logistic model initial test, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_tune
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_2, 
                       newdata = testData, 
                       type = "response")                          # predicted model 
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

```

After running the tuning data against the model, accuracy, TPR and TNR improve slightly. F1 score = `r F1`.

```{r baseline iterated logistic model initial test confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### ROC Analysis

```{r baseline iterative logistic model ROC, eval=T, echo=F, message=F, warning=F}

tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_2, 
                       newdata = tuneData, 
                       type = "response")                          # predicted model 

ROCRpred = ROCR::prediction(Status_pred, tuneData$Status)
ROCRperf = ROCR::performance(ROCRpred, measure="tpr", x.measure="fpr")
AUC <- round(as.numeric(ROCR::performance(ROCRpred, "auc")@y.values), 4)
#print(paste("AUC = ",AUC,sep=""))

```

For this model AUC = `r AUC`. The ROC curve for the tuning dataset is plotted below. 

```{r baseline iterated logistic model ROC plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.5: Receiver operating characteristic (ROC) curve for baseline iterative logistic model fit.'}

plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1))
```

### Threshold Tuning

```{r baseline iterated logistic model tune threshold, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.6: TPR, TNR, accuracy and F1 score vs. threshold.'}

threshold_glm_iterated <- tune_threshold(data = Status_pred, reference = tuneData$Status)

```


```{r baseline iterated logistic model threshold tune, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_2, 
                       newdata = tuneData, 
                       type = "response")                                       # predicted model 
Status_pred[Status_pred < threshold_glm_iterated] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_iterated] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(tuneData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic"
stage <- "tune"
stats_iterglm_tune <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Setting the threshold to `r threshold_glm_iterated` increases TNR to `r TNR`, but decreases accuracy to `r ACC`, F1 score to `r F1` and TPR to `r TPR`. 

```{r baseline iterative logistic model ROC tune confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Test

```{r baseline iterated logistic model verify, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_verify
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_2, 
                       newdata = testData, 
                       type = "response")                                       # predicted model 
Status_pred[Status_pred < threshold_glm_baseline] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_baseline] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic"
stage <- "test"
stats_iterglm_test <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Test data results below show accuracy = `r ACC`, F1 score = `r F1`, TPR = `r TPR` and TNR = `r TNR`. 

```{r baseline iterative logistic model ROC verify confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Summary

```{r baseline iterative logistic model summary, eval=T, echo=F, message=F, warning=F}

model_iterglm <- rbind(stats_iterglm_train, stats_iterglm_tune, stats_iterglm_test, deparse.level=1, make.row.names=F)
colnames(model_iterglm) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_iterglm, caption = 'Table 4.3: Iterated logistic model summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```


```{r baseline iterative logistic model parameter summary plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.7: TPR, TNR, accuracy and F1 score vs. model fit stage'}

myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
               brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    
model_iterglm %>%  
    gather(key="var", value="value", c(3:6)) %>%
    ggplot(aes(factor(Stage, level=c("train","tune","test")), value, group=var, col=var)) +
      geom_line(size=1) +
      geom_point(size=2, aes(color=var)) +
      scale_color_manual(values = myCol) +
      labs(title = "Iterative Logistic Model Performance at Each Stage",
           x = "Model Stages")

```


## Class Rebalancing

A notable characteristic of the SECOM dataset mentioned at the outset of this project was the relatively few number of failure observations compared with passing - less than 1 in 10. This dataset characteristic is known as an imbalanced classification problem and is problematic in machine learning algorithms since they're designed to improve accuracy by minimizing fit error but cannot account for the class distribution. This tends to lead to bias toward the majority class and yields an accurate model with poor discernability to predict minority class observations. Dealing with imbalanced datasets entails strategies for balancing classes in the training data prior to using any machine learning algorithm. One strategy is to use sampling techniques to balance the original dataset. There are a number of resampling techniques in the literature including:

* Undersampling - Randomly removing majority class observations until the classes are balanced.
* Oversampling - Randomly replicate minority class observations until the classes are balanced.
* Informed Oversampling: Synthetic Minority Oversampling Technique (SMOTE) - A subset of minority class data is used to derive, or synthesize new observations.
* Modified Synthetic Minority Oversampling Technique (MSMOTE) - Builds on SMOTE by taking the underlying minority class distribution into account to minimize the effects of noise in the data and blurring of class boundaries.

Another approach to dealing with the class imbalance entails algorithm ensemble techniques to improve the performance of single classifiers of existing classification algorithms. Some examples of techniques in the literature are listed below. These techniques are advanced and a little beyond scope but would be of interest for future development work.

* Bootstrap Aggregating ("Bagging")
* Boosting-Based
* Adaptive Boosting
* Gradient Tree Boosting
* XG Boost

For this work the number of failure observations is too small to consider undersampling, so oversampling is required. SMOTE was chosen as it helps mitigate over fitting errors associated with using multiple copies of identical observations as occurs in statistical oversampling, and the technique is readily implemented using the [**smotefamily package**](https://cran.r-project.org/web/packages/smotefamily/index.html). Table 4.4 below shows the original and balanced training data results. SMOTE is performed only on training data to avoid "leakage" of information into the tune/test data producing misleading results. In order to maintain the predictive integrity of the tuning and test datasets then, those datasets are not rebalanced. Fig. 4.8 overlays training dataset density plots of failure observations before and after rebalancing for a sample of feature distributions and shows that the synthesized data characteristics are consistent with the original as desired. 


```{r smote, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# Status Rebalancing: smotefamily
#-----------------------------------------------------------------------
trainData <- secom_train #[1:50]                                   # Useful for limiting data for debug.

# Once rebalanced values are final, set smoteFlag=F to read them from file.
if (smoteFlag) {                                                   # set flag in setup chunk
    sensorVars <- names(trainData)[grepl("^V",names(trainData)) & !grepl("_NA$",names(trainData))]

    smote_data <- SMOTE(trainData[sensorVars],                     # feature values
                        trainData$Status,                          # class labels
                        K = 20, dup_size = 0)                       # function parameters
    syn_data <- smote_data$syn_data                                # synthetic minority class data
    syn_data <- within(syn_data, rm(class))                        # drop class variable
    syn_data$Status <- 1                                           # all synthesized data are Status = "fail"
   
    secom_smote <- bind_rows(original = trainData,
                             synthetic = syn_data,
                             .id = "data_type")

    # Save imputed dataset for later use.
    path_out <- file.path("Data", "secom_smote.csv")
    write.csv(secom_smote, path_out)
} else {
    path_in <- file.path("Data", "secom_smote.csv")
    secom_smote <- read.csv(path_in, stringsAsFactors=F) 
    secom_smote <- within(secom_smote, rm(X))                      # drop extra variable    
    #secom_smote$Status <- as.factor(secom_smote$Status)
} #end if (smoteFlag)


# Summary Table
train_table <- table(secom_train$Status)
smote_table <- table(secom_smote$Status)
train_state <- c(nrow(secom_train),as.integer(train_table[1]),as.integer(train_table[2]))
smote_state <- c(nrow(secom_smote),as.integer(smote_table[1]),as.integer(smote_table[2]))
status_smote <- as.data.frame(rbind(train_state,smote_state))
colnames(status_smote) <- c("Observations","Pass","Fail")
rownames(status_smote) <- c("Original","Balanced")

#-----------------------------------------------------------------------
# WIP dataset. 
#-----------------------------------------------------------------------
secom_train_smote <- secom_smote                                   # start & end each section with this df

```


```{r smote summary table, echo=F, message=F, warning=F}

kable(status_smote, caption = 'Table 4.4: Taining dataset summary before and after rebalaning.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```

  
```{r sample smote distribution, echo=F, warning=F, message=F, results='hide', out.width='75%', fig.align='center', fig.cap='Fig. 4.8: Example distributions comparing original and synthesized of failure observation distributions.'}
# Review results.
sensorVars <- names(secom_smote)[grepl("^V",names(secom_smote)) & !grepl("_NA$",names(secom_smote))]
#distMatrix(secom_smote, plotVars=sensorVars, factorBy="data_type", plotDim=3, nStart=1, nPlot=9, nBins=100, numXaxes=1)
distMatrix(secom_smote %>% filter(Status == 1), plotVars=sensorVars, factorBy="data_type", plotDim=3, nStart=1, nPlot=9, nBins=100, numXaxes=1, hist=F)
```

## Iterative Logistic Regression Revisited

### Training 

```{r iterative smote logistic model, eval=T, echo=F, message=F, warning=F}
#-----------------------------------------------------------------------
# Logistic Model (iterative coefficient optimization) : bayesglm
#-----------------------------------------------------------------------
trainData <- secom_train_smote 
trainData[c('Date','Time','Yield','data_type')] <- c(NULL)

# Initialize loop variables.
loopCount <- 1                                                         # loop counter
maxPval <- 1                                                           # intial max. p-value
fitStats <- data.frame(loopCount=0,Fstat=0,df=0,Rsq=0)                 # loop stat dataframe
pathname <- "Data/glm_regression_smote_models"                               # base path to glm model files
modelFile <- "fit_glm_baseline_3.rda"
fitConstFile <- "glm_smote_fit_constants.csv"

if(file.exists(file.path(pathname, modelFile))) {
  
    load(file.path(pathname, modelFile))                               # load model
    c <- read.csv(file.path(pathname, fitConstFile), stringsAsFactors=F) # load fit constants
    c <- within(c, rm(X))                                              # drop extra variable    
  
} else {                                                               # else refit model
  
    while (maxPval > maxSig) {
        fit_glm_baseline_3 <- bayesglm(Status ~ . , data=trainData, family="binomial", maxit=200)   # baseline glm fit model
        c <- data.frame(coef(summary(fit_glm_baseline_3)))
        vars <- rownames(c)
        c <- bind_cols(data.frame(vars, stringsAsFactors=F), c)
        c <- c[c(-1),]                                                 # Remove intercept.
        
        df <- summary(fit_glm_baseline_3)$df[1]                        # degrees of freedom
        fitStats[loopCount,] <- c(loopCount, df)
        
        maxPval <- max(abs(c$Pr...z..))                                # current max p-value
        dropVar <- c$vars[which(c(abs(c$Pr...z..) == maxPval))]        # var name with max p-value
        trainData[dropVar] <- NULL
        
        print(paste("Loop#:",loopCount, "  Var. Count =", length(trainData)))
        maxPval <- max(abs(c$Pr...z..))                                # current p-value
        loopCount <- loopCount + 1
    } #end while
    
    save(fit_glm_baseline_3, file=file.path(pathname, modelFile))
    write.csv(c, file.path(pathname, fitConstFile))
    
} # end if file.exists

#Status_pred <- fitted(fit_glm_baseline_3)                          # predicted model vals
Status_pred <- predict(fit_glm_baseline_3)                          # predicted model vals
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Other useful functions 
summary <- summary(fit_glm_baseline_3)                               # summary results
AIC <- round(as.double(summary[5],1))
#coefficients(fit_glm_baseline_3)                                  # model coefficients
#confint(fit_glm_baseline_3, level=0.95)                           # CIs for model parameters 
#fitted(fit_glm_baseline_3)                                        # predicted values
#residuals(fit_glm_baseline_3)                                     # residuals
#anova(fit_glm_baseline_3)                                         # anova table 
#vcov(fit_glm_baseline_3)                                          # covariance matrix for model parameters 
#influence(fit_glm_baseline_3)                                     # regression diagnostics

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(trainData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic (SMOTE)"
stage <- "train"
stats_iterglmsmote_train <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Now that the training dataset is balanced the iterative logistic model is refit. Please refer to Appendix B for detailed model output. The AIC score of `r AIC` is a further improvement with accuracy = `r ACC`, F1 score = `r F1`, TPR = `r TPR` and TNR = `r TNR`. Confusion matrix results and a plot of most to least significant model features are shown below. The number of significant features has substantially increased as shown in Fig. 4.9.

```{r iterative smote logistic model confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```


```{r iterative smote logistic model plots, echo=F, warning=F, message=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.9: Significant model features after class balancing, ordered most to least signicant from left to right.'}

ggplot(c, aes(reorder(vars,Pr...z..), Pr...z..)) +
  geom_point(size=2) +
  scale_y_log10() +
  theme(axis.text.x  = element_text(size = 8, angle = 60, vjust = 0.5)) +
  labs(title = "Significant Model Features",
       x = "Variable Names",
       y = "Probability")

```

After running the tuning data against the model, accuracy, TPR and TNR all decrease. F1 score = `r F1`. 

```{r iterative smote logistic model initial test, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_tune
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_3, 
                       newdata = testData, 
                       type = "response")                          # predicted model 
Status_pred[Status_pred < threshold] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold] <- 1                         # reset "fail" level

# Confusion Matrix
caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))

```

### ROC Analysis

```{r iterative smote logistic model ROC, eval=T, echo=F, message=F, warning=F}

tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_3, 
                       newdata = tuneData, 
                       type = "response")                          # predicted model 

ROCRpred = ROCR::prediction(Status_pred, tuneData$Status)
ROCRperf = ROCR::performance(ROCRpred, measure="tpr", x.measure="fpr")
AUC <- round(as.numeric(ROCR::performance(ROCRpred, "auc")@y.values), 4)
#print(paste("AUC = ",AUC,sep=""))

```

The new AUC = `r AUC` is a decreased showing poorer pass/fail discernability. The ROC curve for the tuning dataset is plotted below. 

```{r iterated smote logistic model ROC plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.10: Receiver operating characteristic (ROC) curve for class balanced, iterative logistic model fit.'}

plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1))
```

### Threshold Tuning

```{r iterated smote logistic model tune threshold, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.11: TPR, TNR, accuracy and F1 score vs. threshold.'}

threshold_glm_smote <- tune_threshold(data = Status_pred, reference = tuneData$Status)

```


```{r iterative smote logistic model threshold tune, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
tuneData <- secom_tune
tuneData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_3, 
                       newdata = tuneData, 
                       type = "response")                                       # predicted model 
Status_pred[Status_pred < threshold_glm_smote] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_smote] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(tuneData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic (SMOTE)"
stage <- "tune"
stats_iterglmsmote_tune <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Setting the threshold to `r threshold_glm_smote` increases TNR to `r TNR`, but decreases accuracy to `r ACC` and TPR to `r TPR`. F1 score = `r F1`.

```{r iterative smote logistic model ROC tune confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Test

```{r iterative smote logistic model verify, eval=T, echo=F, message=F, warning=F}

# Test Set
#testData <- rbind(secom_tune,secom_verify)
testData <- secom_verify
testData[c('Date','Time','data_type')] <- c(NULL)

Status_pred <- predict(fit_glm_baseline_3, 
                       newdata = testData, 
                       type = "response")                                       # predicted model 
Status_pred[Status_pred < threshold_glm_smote] <- 0                          # reset "pass" level
Status_pred[Status_pred >= threshold_glm_smote] <- 1                         # reset "fail" level

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "iterated logistic (SMOTE)"
stage <- "test"
stats_iterglmsmote_test <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Test data results below show accuracy = `r ACC`, TPR = `r TPR`, TNR = `r TNR` and F1 score = `r F1`

```{r iterative smote logistic model ROC verify confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Summary

```{r iterative smote logistic model summary, eval=T, echo=F, message=F, warning=F}

model_iterglmsmote <- rbind(stats_iterglmsmote_train, stats_iterglmsmote_tune, stats_iterglmsmote_test, deparse.level=1, make.row.names=F)
colnames(model_iterglmsmote) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_iterglmsmote, caption = 'Table 4.5: Iterated logistic model summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```


```{r iterative smote logistic model parameter summary plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.12: TPR, TNR, accuracy and F1 score vs. model fit stage'}

myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
               brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    
model_iterglmsmote %>%  
    gather(key="var", value="value", c(3:6)) %>%
    ggplot(aes(factor(Stage, level=c("train","tune","test")), value, group=var, col=var)) +
      geom_line(size=1) +
      geom_point(size=2, aes(color=var)) +
      scale_color_manual(values = myCol) +
      labs(title = "Iterative Logistic (SMOTE) Model Performance at Each Stage",
           x = "Model Stages")

```

## Random Forest

In this last model a random forest classification fit is performed. In short, random forest is a machine learning technique which builds multiple decision trees and merges them together to obtain a more accurate and stable prediction. It's very susceptible to over fitting due to class imbalance which was a primary reason for rebalancing the SECOM dataset originally. The [**randomForest package**](https://cran.r-project.org/web/packages/randomForest/index.html) was used for this project. 

```{r random forest fit matrix, eval=T, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# Random Forest : randomForest
#-----------------------------------------------------------------------
# Training Set
trainData <- secom_train_smote #[1:50]                             # Normalized
trainData[c('Date','Time','data_type')] <- c(NULL)
trainData$Status <- as.factor(trainData$Status)                    # set to factor for classification fit

# Tune Set
tuneData <- secom_tune                                             # Normalized
tuneData[c('Date','Time','data_type')] <- c(NULL)
tuneData$Status <- as.factor(tuneData$Status)                      # set to factor for classification fit

pathname <- "Data/rf_classification_models"                        # base path to rf model files
fitConstFile <- "rf_fit_constants.csv"                             # csv file of RF fit to save time when plotting

# If fitRfFlag = T or fitConstFile doesn't exist, run RF fit routine.
if(!(file.exists(file.path(pathname, fitConstFile))) | fitRfFlag) {
    
    # Define empty stats dataframe.
    rf_fit_stats <- data.frame(nodesize=integer(),
                               ntree=integer(),
                               maxit=integer(),
                               mtry=integer(),
                               cutoff=double(),
                               RMSE_train=double(),
                               TPR_train=double(),
                               TNR_train=double(),
                               PPV_train=double(),
                               NPV_train=double(),
                               ACC_train=double(),
                               BALACC_train=double(),
                               F1_train=double(),
                               F2_train=double(),
                               RMSE_tune=double(),
                               TPR_tune=double(),
                               TNR_tune=double(),
                               PPV_tune=double(),
                               NPV_tune=double(),
                               ACC_tune=double(),
                               BALACC_tune=double(),
                               F1_tune=double(),
                               F2_tune=double(),
                               stringsAsFactors=FALSE)
    
    # Optimize using broad-to-narrow grids.
    numPasses <- 2                                                     # Number of grids to run.
    for (passNum in 1:numPasses) {
        #-------------------------------
        # RF tuning parameters
        #-------------------------------
        # Coarse grid.
        if (passNum == 1) {
            nodesizes <- c(1,4,10,40,100)
            ntrees <- c(10,40,100,400,1000)
            maxits <- c(10) 
            mtrys <- c(1,4,10,40,100,400)
            cutoffs <- c(0.20,0.25,0.30,0.35,0.40,0.45,0.50)
        } #end if (passNum == 1)
        
        # Finer cutoff grid.
        if (passNum == 2) {
            nodesizes <- c(1,4,10,40,100)
            ntrees <- c(10,40,100,400,1000)
            maxits <- c(10) 
            mtrys <- c(1,4,10,40,100,400)
            cutoffs <- c(0.23,0.24,0.26,0.27,0.28,0.29)
        } #end if (passNum == 2)
        
        # mtry fine step.
        if (passNum == 3) {
            nodesizes <- c(4)
            ntrees <- c(400)
            maxits <- c(10) 
            mtrys <- c(1:400)
            cutoffs <- c(0.25)
        } #end if (passNum == 3)
        
        # nodesize fine step.
        if (passNum == 4) {
            nodesizes <- c(1:100)
            ntrees <- c(400)
            maxits <- c(10) 
            mtrys <- c(40)
            cutoffs <- c(0.25)
        } #end if (passNum == 4)
        
        for (nodesize in nodesizes) {
            for (ntree in ntrees) {
                for (maxit in maxits) {
                    for (mtry in mtrys) {
                        for (cutoff in cutoffs) {
                            fileName <- paste(pathname,"/rf_nodesize=",nodesize,"_ntree=",ntree,"_maxit=",maxit,"_mtry=",mtry,"_cutoff=",cutoff,".rda", sep="")
                            
                            if(file.exists(fileName)) {
                                
                                load(fileName)                                 # load model
                                 
                            } else {                                           # else refit model
                                 
                                fit_forest <- randomForest(Status ~ ., data = trainData, 
                                                           nodesize = nodesize, 
                                                           ntree = ntree,
                                                           maxit = maxit,
                                                           mtry = mtry,
                                                           importance = TRUE,
                                                           keep.forest = TRUE,
                                                           cutoff = c(1-cutoff,cutoff))
                                
                                save(fit_forest, file = fileName)
                            } #end if
                        
                            # Training Fit Errors
                            Status_train <- predict(fit_forest, newdata = trainData)              # predicted model vals
                            if (!is.factor(trainData$Status)) {                                  # doesn't apply for classification
                                Status_train[Status_train < threshold] <- 0                      # reset "pass" level
                                Status_train[Status_train >= threshold] <- 1                     # reset "fail" level
                            } #end if
                            RMSE_train <- mean((as.numeric(trainData$Status) - as.numeric(Status_train))^2)
                            
                            cm <- table(data = as.factor(Status_train), reference = as.factor(trainData$Status))
                            TP <- cm[1,1]                                 # true positive
                            FN <- cm[2,1]                                 # false positive
                            FP <- cm[1,2]                                 # false negative
                            TN <- cm[2,2]                                 # true negative
                            CP <- TP + FN                                 # condition positive (data)
                            CN <- TN + FP                                 # condition negative (data)
                            MP <- TP + FP                                 # predicted positive (model)
                            MN <- TN + FN                                 # predicted negative (model)
                            TPR_train <- TP/CP                            # true positive rate (sensitivity, recall)
                            TNR_train <- TN/CN                            # true negative rate (specificity)
                            PPV_train <- TP/MP                            # positive predictive value (precision)
                            NPV_train <- TN/MN                            # negative predictive value
                            ACC_train <- (TP + TN)/(CP + CN)              # accuracy
                            BALACC_train <- (TP/CP + TN/CN)/2             # balanced accuracy
                            F1_train <- 2*(TP/MP)*(TP/CP)/(TP/MP + TP/CP) # F1 score = 2*precision*recall/(precision + recall)
                            F2_train <- 2*(TN/MN)*(TN/CN)/(TN/MN + TN/CN) # F2 score = 2*NPV*specificity/(NPV + specificity)
                            
                            # Tune Fit Errors
                            Status_tune <- predict(fit_forest, newdata = tuneData)              # predicted model vals
                            if (!is.factor(tuneData$Status)) {                                  # doesn't apply for classification
                                Status_tune[Status_tune < threshold] <- 0                       # reset "pass" level
                                Status_tune[Status_tune >= threshold] <- 1                      # reset "fail" level
                            } #end if
                            RMSE_tune <- mean((as.numeric(tuneData$Status) - as.numeric(Status_tune))^2)
        
                            cm <- table(data = as.factor(Status_tune), reference = as.factor(tuneData$Status))
                            TP <- cm[1,1]                                 # true positive
                            FN <- cm[2,1]                                 # false positive
                            FP <- cm[1,2]                                 # false negative
                            TN <- cm[2,2]                                 # true negative
                            CP <- TP + FN                                 # condition positive (data)
                            CN <- TN + FP                                 # condition negative (data)
                            MP <- TP + FP                                 # predicted positive (model)
                            MN <- TN + FN                                 # predicted negative (model)
                            TPR_tune <- TP/CP                             # true positive rate (sensitivity, recall)
                            TNR_tune <- TN/CN                             # true negative rate (specificity)
                            PPV_tune <- TP/MP                             # positive predictive value (precision)
                            NPV_tune <- TN/MN                             # negative predictive value
                            ACC_tune <- (TP + TN)/(CP + CN)               # accuracy
                            BALACC_tune <- (TP/CP + TN/CN)/2              # balanced accuracy
                            F1_tune <- 2*(TP/MP)*(TP/CP)/(TP/MP + TP/CP)  # F1 score = 2*precision*recall/(precision + recall)
                            F2_tune <- 2*(TN/MN)*(TN/CN)/(TN/MN + TN/CN)  # F2 score = 2*NPV*specificity/(NPV + specificity)
                            
                            # Model Fit Stats
                            rf_fit_stats <- base::rbind(rf_fit_stats, 
                                                        c(as.integer(nodesize),as.integer(ntree),as.integer(maxit),as.integer(mtry),cutoff,
                                                          RMSE_train,TPR_train,TNR_train,PPV_train,NPV_train,
                                                          ACC_train,BALACC_train,F1_train,F2_train,
                                                          RMSE_tune,TPR_tune,TNR_tune,PPV_tune,NPV_tune,
                                                          ACC_tune,BALACC_tune,F1_tune,F2_tune),
                                                          deparse.level = 1,
                                                          make.row.names = F)
                            
                        } #end for (cutoff in cutoffs)
                    } #end for (mtry in mtrys)
                } #end for (maxit in maxits)
            } #end for (ntree in ntrees)
        } #end for (nodesize in nodesizes)
    } #end for (passNum in 1:numPasses) 
    
    colnames(rf_fit_stats) <- c("nodesize","ntree","maxit","mtry","cutoff",
                                "RMSE_train","TPR_train","TNR_train","PPV_train","NPV_train",
                                "ACC_train","BALACC_train","F1_train","F2_train",
                                "RMSE_tune","TPR_tune","TNR_tune","PPV_tune","NPV_tune",
                                "ACC_tune","BALACC_tune","F1_tune","F2_tune")
    rf_fit_stats$nodesize  <- as.integer(rf_fit_stats$nodesize)
    rf_fit_stats$ntree     <- as.integer(rf_fit_stats$ntree)
    rf_fit_stats$maxit     <- as.integer(rf_fit_stats$maxit)
    rf_fit_stats$mtry      <- as.integer(rf_fit_stats$mtry)
    
    # Save dataset for later use.
    path_out <- file.path(pathname, fitConstFile)
    write.csv(rf_fit_stats, path_out)
    print("Saving RF stats file.")

} else {
    
    rf_fit_stats <- read.csv(file.path(pathname, fitConstFile), stringsAsFactors=F) # load fit constants
    rf_fit_stats <- within(rf_fit_stats, rm(X))                                     # drop extra variable   
    print("Reading RF stats file.")
    
}
```

### Parameter Analysis

The random forest model contains a number of tuning variables to tweak the model of which the primary ones considered here were 'cutoff', 'ntree', 'mtry', 'nodesize' and 'maxit'. The approach taken was to build a parameter grid to measure the model sensitivity for TPR, TNR, accuracy and F1 score. Initially max iterations (maxit) was found to be insensitive so the remaining results assume maxit = 10 for the sake of speed. The plots below show TPR, TNR, accuracy and F1 score versus cutoff with ntree as the parameter where ntree is the number of trees to grow in the forest for the training data. The 'cutoff' parameter functions similar to the threshold for earlier logistic models. The plots are faceted by 'mtry', the number of variables randomly sampled as candidates at each split, and 'nodesize', the minimum size of terminal nodes. Results for the training data are shown below where it's seen that the number of trees (ntree) has little effect for ntree > 100 due to the 'cutoff' settings restricting over fitting. The training results seem to suggest a wide range of values for 'cutoff', 'mtry' and 'nodesize' for obtaining a perfect fit. 

```{r rf train - coarse grid cutoff plots, eval=T, echo=F, message=F, warning=F, results='hide', out.width='50%', fig.align='left', fig.show='hold', fig.cap='Fig. 4.13 Training data True Positive Rate (TPR), True Negative Rate (TNR), Accuracy (ACC) and F1 Score (F1) vs. cutoff coarse grid plots.'}

#------------------------------
# optimize cutoff
#------------------------------
plotFitStats(rf_fit_stats %>% 
             filter(ntree >= 10, 
                    cutoff %in% c(0.20,0.25,0.30,0.35,0.40,0.45,0.50),
                    mtry %in% c(1,4,10,40,100,400)),
           yList      = c("TPR_train","TNR_train","ACC_train","F1_train"),
           xList      = c("cutoff"  ),
           byVarList  = c("ntree"   ),
           xFacetList = c("mtry"    ),
           yFacetList = c("nodesize"),
           xScale="lin")

```

### Parameter Optimization

Results for the tuning dataset in Fig. 4.14 below exhibit significantly more dependence on 'cutoff', 'mtry' and 'nodesize'. The most significant dependence change is with TNR which is now inversely proportional to 'cutoff' and  significantly decreases the model's efficacy. Similar to the method used to identify the optimal threshold value for the earlier logistic models, Fig. 4.15 shows the relationship for TNR, TPR, accuracy and F1 score versus 'cutoff' overlaid on the same plots and faceted again by 'mtry' and 'nodesize'. Using the magnified plot on the right side, the optimal model parameters maximize the intersection of these 4 measures which are listed below.

* nodesize = 1 
* ntree = 400
* maxit = 10
* mtry = 40
* cutoff = 0.25

```{r rf tune - coarse grid cutoff plots, eval=T, echo=F, message=F, warning=F, results='hide', out.width='50%', fig.align='left', fig.show='hold', fig.cap='Fig. 4.14 Tuning data True Positive Rate (TPR), True Negative Rate (TNR), Accuracy (ACC) and F1 Score (F1) vs. cutoff coarse grid plots.'}

#------------------------------
# optimize cutoff
#------------------------------
plotFitStats(rf_fit_stats %>% 
             filter(ntree >= 10, 
                    cutoff %in% c(0.20,0.25,0.30,0.35,0.40,0.45,0.50),
                    mtry %in% c(1,4,10,40,100,400)),
           yList      = c("TPR_tune","TNR_tune","ACC_tune","F1_tune"),
           xList      = c("cutoff"  ),
           byVarList  = c("ntree"   ),
           xFacetList = c("mtry"    ),
           yFacetList = c("nodesize"),
           xScale="lin")

```


```{r rf coarse grid cutoff plots, eval=T, echo=F, message=F, warning=F, results='hide', out.width='50%', fig.align='left', fig.show='hold', fig.cap='Fig. 4.15 Tuning data True Positive Rate (TPR), True Negative Rate (TNR), Accuracy (ACC) and F1 Score (F1) vs. cutoff overlay plots.'}

# Test Classification Stats vs. cutoff
plotFitStats(rf_fit_stats %>% filter(ntree == 400,
                         cutoff %in% c(0.20,0.25,0.30,0.35,0.40,0.45,0.50),
                         mtry %in% c(1,4,10,40,100,400)),
           yList      = c("Classification Stats"),
           xList      = c("cutoff"  ),
           byVarList  = c("ntree"   ),
           xFacetList = c("mtry"    ),
           yFacetList = c("nodesize"),
           gatherList = c("TPR_tune","TNR_tune","ACC_tune","F1_tune"),
           xScale = "lin")
           
# Tune Classification Stats vs. cutoff
plotFitStats(rf_fit_stats %>% filter(ntree == 400,
                         cutoff >= 0.23, cutoff <= 0.3,
                         mtry %in% c(1,4,10,40,100,400)),
           yList      = c("Classification Stats"),
           xList      = c("cutoff"  ),
           byVarList  = c("ntree"   ),
           xFacetList = c("mtry"    ),
           yFacetList = c("nodesize"),
           gatherList = c("TPR_tune","TNR_tune","ACC_tune","F1_tune"),
           xScale = "lin")
           
# Best overall accuracy at even TPR & TNR
rf_fit_stats[which(rf_fit_stats$TNR_tune > 0.8 & rf_fit_stats$TPR_tune > 0.8),]
```

### Optimum Model (Training Data)

```{r random forest training model, eval=T, echo=F, message=F, warning=F, results='hide'}
#-----------------------------------------------------------------------
# Random Forest : randomForest
#-----------------------------------------------------------------------
# Training Set
trainData <- secom_train_smote #[1:50]                               # Normalized
trainData[c('Date','Time','data_type')] <- c(NULL)
trainData$Status <- as.factor(trainData$Status)                        # set to factor for classification fit

# Select optimum model parameters per above analysis.
nodesize = 1 
ntree = 400
maxit = 10
mtry = 40
cutoff = 0.25


fileName <- paste("Data/rf_classification_models/rf_nodesize=",nodesize,"_ntree=",ntree,"_maxit=",maxit,"_mtry=",mtry,"_cutoff=",cutoff,".rda", sep="")

if (file.exists(fileName)) {
    load(fileName) 
    print("Model file loaded.")
} else {
    fit_forest <- randomForest(Status ~ ., data = trainData, 
                               nodesize = nodesize, 
                               ntree = ntree,
                               maxit = maxit,
                               mtry = mtry,
                               importance = TRUE,
                               keep.forest = TRUE,
                               #classwt = c(6e-3,6e3),
                               cutoff = c(1-cutoff,cutoff))
}

# Explore Model
#plot(fit_forest,
#     main = "Random Forest")

#varImpPlot(fit_forest,
#     cex = 0.7,
#     pt.cex = 1.5,
#     pch = 20,
#     color = "blue",
#     main = "Random Forest Variable Importance")

Status_train <- predict(fit_forest, 
                       newdata = trainData,
                       type = "response")                          # predicted response 

if (!is.factor(trainData$Status)) {                                  # doesn't apply for classification
    Status_train[Status_train < threshold] <- 0                      # reset "pass" level
    Status_train[Status_train >= threshold] <- 1                     # reset "fail" level
} #end if

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_train), reference = as.factor(trainData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "random forest"
stage <- "train"
stats_rf_train <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Training data results below show accuracy = `r ACC`, TPR = `r TPR`, TNR = `r TNR` and F1 score = `r F1`

```{r random forest training confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```


### Optimum Model (Tuning Data)

```{r random forest tune, eval=T, echo=F, message=F, warning=F, results='hide'}
# Test Set
tuneData <- secom_tune                                              # Useful for limiting data for debug.
tuneData[c('Date','Time','data_type')] <- c(NULL)
tuneData$Status <- as.factor(tuneData$Status)                        # set to factor for classification fit
                  
Status_tune <- predict(fit_forest,
                       newdata = tuneData,
                       type = "response")                          # predicted model vals

if (!is.factor(tuneData$Status)) {                                  # doesn't apply for classification
    Status_tune[Status_tune < threshold] <- 0                      # reset "pass" level
    Status_tune[Status_tune >= threshold] <- 1                     # reset "fail" level
} #end if

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_tune), reference = as.factor(tuneData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "random forest"
stage <- "tune"
stats_rf_tune <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Tuning data results below show accuracy = `r ACC`, TPR = `r TPR`, TNR = `r TNR` and F1 score = `r F1`

```{r random forest tuning confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Optimum Model (Test Data)

```{r random forest test, eval=T, echo=F, message=F, warning=F, results='hide'}
# Test Set
testData <- secom_verify                                              # Useful for limiting data for debug.
testData[c('Date','Time','data_type')] <- c(NULL)
testData$Status <- as.factor(testData$Status)                        # set to factor for classification fit
                  
Status_pred <- predict(fit_forest,
                       newdata = testData,
                       type = "response")                          # predicted model vals

if (!is.factor(testData$Status)) {                                  # doesn't apply for classification
    Status_pred[Status_pred < threshold] <- 0                      # reset "pass" level
    Status_pred[Status_pred >= threshold] <- 1                     # reset "fail" level
} #end if

# Confusion Matrix
cm <- caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(testData$Status))
ACC <- round(cm$overall[1],4)                                      # accuracy
TPR <- round(cm$byClass[1],4)                                      # true positive rate (sensitivity,recall)
TNR <- round(cm$byClass[2],4)                                      # true negative rate (specificity)
F1  <- round(cm$byClass[7],4)                                      # F1 score

# Fit Results
model <- "random forest"
stage <- "test"
stats_rf_test <- data.frame(model,stage,TPR,TNR,ACC,F1, stringsAsFactors=F, row.names=NULL)

```

Test data results below show accuracy = `r ACC`, TPR = `r TPR`, TNR = `r TNR` and F1 score = `r F1`

```{r random forest test confusion matrix, eval=T, echo=F, message=F, warning=F}
cm
```

### Optimum Model (Summary)

```{r random forest model summary, eval=T, echo=F, message=F, warning=F}

model_rf <- rbind(stats_rf_train, stats_rf_tune, stats_rf_test, deparse.level=1, make.row.names=F)
colnames(model_rf) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_rf, caption = 'Table 4.6: Random Forest model summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

```


```{r random forest model parameter summary plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.16: TPR, TNR, accuracy and F1 score vs. model fit stage'}

myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
               brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    
model_rf %>%  
    gather(key="var", value="value", c(3:6)) %>%
    ggplot(aes(factor(Stage, level=c("train","tune","test")), value, group=var, col=var)) +
      geom_line(size=1) +
      geom_point(size=2, aes(color=var)) +
      scale_color_manual(values = myCol) +
      labs(title = "Random Forest Model Performance at Each Stage",
           x = "Model Stages")

```
## Model Summaries

Model summaries are provide below and grouped by the modeling stage, either train, tune or test. The test stage is considered the final model performance. 

### Training Summary

Table 4.7 summarizes TPR, TNR, accuracy and F1 score for the models considered in this project at training stage. All models score very high but random forest results suggest a perfect fit. This is due to the nature of random forest to over fit the data and why unseen data sets are then used for tuning and final test results. 

```{r model training summary, eval=T, echo=F, message=F, warning=F}

model_train <- rbind(stats_glm_train, stats_iterglm_train, stats_iterglmsmote_train, stats_rf_train, deparse.level=1, make.row.names=F)
colnames(model_train) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_train, caption = 'Table 4.7: Model training summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

### Tuning Summary

Table 4.8 summarizes TPR, TNR, accuracy and F1 score for the models considered in this project at tuning stage. All model performances decay nearly the same amount but results suggest that the iterated logistic model provides the best overall performance with random forest right behind.

```{r model tuning summary, eval=T, echo=F, message=F, warning=F}

model_tune <- rbind(stats_glm_tune, stats_iterglm_tune, stats_iterglmsmote_tune, stats_rf_tune, deparse.level=1, make.row.names=F)
colnames(model_tune) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_tune, caption = 'Table 4.8: Model tuning summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

### Final Test Summary

Table 4.9 summarizes TPR, TNR, accuracy and F1 score for the models considered in this project at the final testing stage. There was a notable shift in the iterated logistic model due to threshold sensitivity. The remaining three models remained comparably stable. The baseline logistic and class balanced iterative logistic model appear to perform identically and exhibit ~10% overall performance boost compared with tuning data results. Between these two the balanced iterative logistic model is preferred as it has fewer features and the lower AIC score. However, the random forest model results are not far behind and in fact have less variability comparing tuning with test results. Random forest is therefore a very good alternative model. 

```{r model test summary, eval=T, echo=F, message=F, warning=F}

model_test <- rbind(stats_glm_test, stats_iterglm_test, stats_iterglmsmote_test, stats_rf_test, deparse.level=1, make.row.names=F)
colnames(model_test) <- c("Model","Stage","TPR","TNR","Accuracy","F1-Score")
kable(model_test, caption = 'Table 4.9: Model final test summary.') %>%
  kable_styling(bootstrap_options = "striped", full_width = F)

# Save summary results.
path_out <- file.path("Data", "summary.csv")
write.csv(model_test, path_out)

```


```{r model test parameter summary plot, eval=T, echo=F, message=F, warning=F, results='hide', out.width='75%', fig.align='center', fig.show='hold', fig.cap='Fig. 4.17: TPR, TNR, accuracy and F1 score vs. model.'}

myCol <- rbind(brewer.pal(9, "Blues")[c(3,5,6,8,9)],
               brewer.pal(9, "Reds")[c(3,5,6,8,9)])
    
model_test %>%  
    gather(key="var", value="value", c(3:6)) %>%
    ggplot(aes(factor(Model, level=c("logistic","iterated logistic","iterated logistic (SMOTE)","random forest")), value, group=var, col=var)) +
      geom_line(size=1) +
      geom_point(size=2, aes(color=var)) +
      scale_color_manual(values = myCol) +
      labs(title = "Final Test Model Performance",
           x = "Model Stages")

```
\newline
\pagebreak

# Conclusions and Future Recommendations

## Conclusions

* The baseline logistic model yielded an accuracy of TPR = `r model_test[1,3]`, TNR = `r model_test[1,4]`, `r 100*model_test[1,5]`%, F1 score = `r 100*model_test[1,6]`% but used all `r as.numeric(fit_glm_baseline_1[6]) - 1` features.
* The more efficient model with this same predictive power as the baseline model was the iterated logistic model applied to class balanced data which yielded an accuracy of TPR = `r model_test[3,3]`, TNR = `r model_test[3,4]`, `r 100*model_test[3,5]`%, F1 score = `r 100*model_test[3,6]`% and used only `r as.numeric(fit_glm_baseline_3[6]) - 1` features.
* The random forest model performance was slightly lower than the above logistic models, but the stability in the model from tuning to test suggest the model is a very good runner-up.

## Future Recommendations

* The power and utility of random forest was evident in the initial training results, but even using class balanced data and optimizing on the 'cutoff' parameter, the model still suffered from over fitting. The next step for this dataset is to apply algorithm ensemble techniques mentioned earlier for addressing class imbalance. In particular XG Boost, an extreme gradient boost algorithm based on trees, is of interest due to its out-of-the-box performance, versatility in tuning parameters and parallel processing capability.
* The biggest problem with the current dataset was not having any information on the sensor origins. If the physical meaning of the measurements were available it would be possible to perform feature engineering to properly transform the data based on the underlying physical meaning. 
* The pass/fail criteria for the current dataset is another unknown. If this criteria was available it would provide additional feature engineering support or suggest additional sub classifications, meaning there may be more than one failure criteria at work which affects which sensors are more relevant for each failure type. The current model assumes the same sensors are important for all failures which is not correct. 

```{r experimental train random forest, eval=F, echo=F, message=F, warning=F}
trainData <- secom_train_smote
trainData[c('Date','Time','data_type')] <- c(NULL)
#trainData$Status <- as.factor(trainData$Status)                    # set to factor for classification fit

# Test Set
tuneData <- secom_tune                                             # Normalized
tuneData[c('Date','Time','data_type')] <- c(NULL)
#tuneData$Status <- as.factor(tuneData$Status)                      # set to factor for classification fit

#set.seed(949)

#fitControl <- trainControl(method = "cv",
#                           classProbs = TRUE,
#                           summaryFunction = twoClassSummary)

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           classProbs = TRUE)

mod1 <- train(Status ~ ., data = trainData,
               method = "rf",
               metric = "ROC",
               tuneGrid = data.frame(mtry = 3),
               trControl = fitControl)

getTrainPerf(mod0)



# Predicted Training Results
Status_pred <- predict(mod0,
                       newdata = trainData)                          # predicted model vals
#if (!is.factor(tuneData$Status)) {                                  # doesn't apply for classification
#    Status_pred[Status_pred < threshold] <- 0                      # reset "pass" level
#    Status_pred[Status_pred >= threshold] <- 1                     # reset "fail" level
#} #end if

pred = ROCR::prediction(Status_pred, trainData$Status)
AUC <- as.numeric(ROCR::performance(pred, "auc")@y.values)
AUC

ROCRpred = ROCR::prediction(Status_pred, trainData$Status)
ROCRperf = ROCR::performance(ROCRpred, measure="sens", x.measure="spec")

plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1))

# Confusion Matrix
caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(trainData$Status))

# Predicted Test Results
Status_pred <- predict(mod0,
                       newdata = tuneData)                          # predicted model vals
#if (!is.factor(tuneData$Status)) {                                  # doesn't apply for classification
#    Status_pred[Status_pred < threshold] <- 0                      # reset "pass" level
#    Status_pred[Status_pred >= threshold] <- 1                     # reset "fail" level
#} #end if

pred = ROCR::prediction(Status_pred, tuneData$Status)
AUC <- as.numeric(ROCR::performance(pred, "auc")@y.values)
AUC

ROCRpred = ROCR::prediction(Status_pred, tuneData$Status)
ROCRperf = ROCR::performance(ROCRpred, measure="sens", x.measure="spec")

plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1))

# Confusion Matrix
caret::confusionMatrix(data = as.factor(Status_pred), reference = as.factor(tuneData$Status))

```
\newline
\pagebreak

# Appendix A 

## Initial Variable Distributions

```{r initial distributions and QQ plots, eval=T, echo=F, warning=F, message=F, results='hide', out.width='100%', fig.align='center', fig.show='hold', fig.cap='Fig. A.1: Initial variable density and QQ plots.'}

# Density + Q-Q Plots
#plot_data <- secom_trans
plot_data <- secom
sensorVars <- names(plot_data)[grepl("^V",names(plot_data)) & !grepl("_NA$",names(plot_data))]
#distMatrix(plot_data, plotVars=sensorVars, plotDim=3, nStart=1, nPlot=9, nBins=100, QQplot=TRUE)
distMatrix(plot_data, plotVars=sensorVars, factorBy="Status", plotDim=3, nStart=1, nBins=100, QQplot=TRUE)
```
\newline
\pagebreak

## Final Transformed Variable Distributions

```{r final distributions and QQ plots, eval=T, echo=F, warning=F, message=F, results='hide', out.width='100%', fig.align='center', fig.show='hold', fig.cap='Fig. B.1: Final transformed variable density and QQ plots.'}

# Density + Q-Q Plots
#plot_data <- secom_trans
plot_data <- secom_imp
sensorVars <- names(plot_data)[grepl("^V",names(plot_data)) & !grepl("_NA$",names(plot_data))]
#distMatrix(plot_data, plotVars=sensorVars, plotDim=3, nStart=1, nPlot=9, nBins=100, QQplot=TRUE)
distMatrix(plot_data, plotVars=sensorVars, factorBy="Status", plotDim=3, nStart=1, nBins=100, QQplot=TRUE)
```
\newline
\pagebreak

# Appendix B

## Baseline Logistic Regression Model Results

```{r baseline logistic regression model results, eval=T, echo=F, warning=F, message=F, fig.cap='Fig. B.1: Final transformed variable density and QQ plots.'}

summary(fit_glm_baseline_1)

```
\newline
\pagebreak

## Iterative Logistic Regression Model Results

```{r iterative logistic regression model results, eval=T, echo=F, warning=F, message=F, fig.cap='Fig. B.1: Final transformed variable density and QQ plots.'}

summary(fit_glm_baseline_2)

```
\newline
\pagebreak

## Class Balanced Iterative Logistic Regression Model Results

```{r smote balanced iterative logistic regression model results, eval=T, echo=F, warning=F, message=F, fig.cap='Fig. B.1: Final transformed variable density and QQ plots.'}

summary(fit_glm_baseline_3)

```

